{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Translator\n",
    "\n",
    "Based on: https://huggingface.co/docs/transformers/tasks/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, datetime, copy\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import TranslationPipeline\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, Split, ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers.pre_tokenizers\n",
    "import tokenizers.processors\n",
    "import tokenizers.decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdli\n",
    "import languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"PretrainTranslator.ipynb\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "source_langs = set([\"akk\", \"sux\"])\n",
    "\n",
    "# target_langs = set([\"en\", \"it\", \"es\", \"fr\", \"de\"])\n",
    "target_langs = set([\"en\"])\n",
    "\n",
    "base_model_id = \"t5-base\"\n",
    "\n",
    "max_vocab_size = 50_000\n",
    "model_max_length = 512\n",
    "batch_size = 8 if os.path.basename(base_model_id).startswith(\"t5-base\") else 128\n",
    "\n",
    "num_train_sequences = 524_288 * 128\n",
    "num_warmup_sequences = 10_000 * 128\n",
    "\n",
    "warmup_learning_rate = 0.01\n",
    "\n",
    "use_paragraphs = True\n",
    "use_lines = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-base-pre-p-l-akksux-en-20220726-154724'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "flags = \"\"\n",
    "suffix = \"\"\n",
    "if use_paragraphs:\n",
    "    flags += \"-p\"\n",
    "if use_lines:\n",
    "    flags += \"-l\"\n",
    "model_id = f\"{os.path.basename(base_model_id)}-pre{flags}-{''.join(sorted(list(source_langs)))}-{''.join(sorted(list(target_langs)))}-{date_id}{suffix}\"\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, <torch.cuda.device at 0x7f02435ee3e0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.cuda.device(0) if has_cuda else \"cpu\"\n",
    "has_cuda, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 26 15:47:24 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 30%   49C    P8    30W / 350W |    168MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       971      G   /usr/lib/xorg/Xorg                120MiB |\r\n",
      "|    0   N/A  N/A      1236      G   ...ome-remote-desktop-daemon        4MiB |\r\n",
      "|    0   N/A  N/A      1272      G   /usr/bin/gnome-shell               23MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_src_chars_per_token = 2.6712177445735397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate Sumerian to Spanish: '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prefix(src_lang, tgt_lang):\n",
    "    s = languages.all_languages[src_lang]\n",
    "    t = languages.all_languages[tgt_lang]\n",
    "    return f\"translate {s} to {t}: \"\n",
    "    \n",
    "get_prefix(\"suxts\", \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/cdli-gh/data/raw/master/cdliatf_unblocked.atf\n",
      "Parsing atf\n"
     ]
    }
   ],
   "source": [
    "publications = cdli.get_atf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134712, 'publications')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publications), \"publications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False ''\n",
      "False ' '\n",
      "False 'xx xxx x'\n",
      "False '.. . .. '\n",
      "True 'Hi'\n"
     ]
    }
   ],
   "source": [
    "def target_ok(target_text):\n",
    "    if len(target_text) == 0:\n",
    "        return False\n",
    "    if len(set(target_text.replace(\" \", \"\"))) < 2:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "def test_target_ok(text):\n",
    "    ok = target_ok(text)\n",
    "    print(ok, repr(text))\n",
    "    \n",
    "test_target_ok(\"\")\n",
    "test_target_ok(\" \")\n",
    "test_target_ok(\"xx xxx x\")\n",
    "test_target_ok(\".. . .. \")\n",
    "test_target_ok(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wmax_num_tokens = model_max_length - 192\n",
    "\n",
    "def wrap_paragraph(paragraph, lines, src_lang, tgt_lang):\n",
    "    ptag, pline_start_index, pline_end_index = paragraph\n",
    "    wline_ranges = []\n",
    "    wline_tok_len = 0.0\n",
    "    \n",
    "    def start_new_line(pline_index):\n",
    "#         print(\"start\", pline_index)\n",
    "        wline_ranges.append((pline_index, pline_index + 1))\n",
    "        \n",
    "    def append_line(pline_index):\n",
    "#         print(\"append\", pline_index)\n",
    "        r = wline_ranges[-1]\n",
    "        if r[1] == pline_index:\n",
    "            wline_ranges[-1] = (r[0], r[1] + 1)\n",
    "        else:\n",
    "            print(f\"Missing line: got {pline_index}, expected {r[1]}: {wline_ranges}\")\n",
    "\n",
    "    for pline_index in range(pline_start_index, pline_end_index):\n",
    "        pline_num_toks = len(lines[pline_index].text) / avg_src_chars_per_token + 1.0\n",
    "        if len(wline_ranges) == 0 or (wline_tok_len + pline_num_toks > wmax_num_tokens):\n",
    "            start_new_line(pline_index)\n",
    "            wline_tok_len = 0.0\n",
    "        else:\n",
    "            append_line(pline_index)\n",
    "        wline_tok_len += pline_num_toks\n",
    "    return wline_ranges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['akk', 'sux'])\n"
     ]
    }
   ],
   "source": [
    "dataset_index = json.load(open(\"../data/dataset_index.json\", \"rt\"))\n",
    "print(dataset_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 akk train\n",
      "108 akk test\n",
      "3753 sux train\n",
      "396 sux test\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_index[\"akk\"][\"train\"]), \"akk train\")\n",
    "print(len(dataset_index[\"akk\"][\"test\"]), \"akk test\")\n",
    "print(len(dataset_index[\"sux\"][\"train\"]), \"sux train\")\n",
    "print(len(dataset_index[\"sux\"][\"test\"]), \"sux test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sux to en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99456/99456 [00:00<00:00, 253366.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing akk to en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21820/21820 [00:00<00:00, 198020.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "146812"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pubs_sources():\n",
    "\n",
    "    added_sources = set()\n",
    "\n",
    "    def add_line_ranges(s, area, b, e):\n",
    "        ls = \" \".join([x.text for x in area.lines[b:e]])\n",
    "        ls = (s, \" \".join(ls.split(\" \")))\n",
    "        added_sources.add(ls)\n",
    "        for t in target_langs:\n",
    "            lt = \" \".join([(x.languages[t] if t in x.languages else \"\") for x in area.lines[b:e]])\n",
    "            lt = (t, \" \".join(lt.split(\" \")))\n",
    "            added_sources.add(lt)\n",
    "\n",
    "    for s in source_langs:\n",
    "        for t in target_langs:\n",
    "            print(\"Preparing\", s, \"to\", t)            \n",
    "            st_prefix = get_prefix(s, t)\n",
    "            ts_prefix = get_prefix(t, s)\n",
    "            for pub in tqdm([p for p in publications if p.language==s]):\n",
    "                for area in pub.text_areas:\n",
    "                    if not any(x for x in area.lines if t in x.languages):\n",
    "                        continue\n",
    "                    if use_paragraphs:\n",
    "                        paragraphs = area.lines_to_paragraphs(s)\n",
    "                        line_ranges = []                \n",
    "                        for p in paragraphs:                    \n",
    "                            wlines = wrap_paragraph(p, area.lines, s, t)\n",
    "                            line_ranges.extend(wlines)\n",
    "        #                 print(\"=\"*50, len(area.lines))\n",
    "                        for b, e in line_ranges:\n",
    "                            add_line_ranges(s, area, b, e)\n",
    "                    if use_lines:\n",
    "                        for i, _ in enumerate(area.lines):\n",
    "                            add_line_ranges(s, area, i, i + 1)\n",
    "#     random.shuffle(new_sourceandtargets)\n",
    "#     return Dataset.from_dict({\"source\": [x[0] for x in new_sourceandtargets], \"target\": [x[1] for x in new_sourceandtargets]})\n",
    "    return added_sources\n",
    "\n",
    "all_sources = get_pubs_sources()\n",
    "all_sources = list(all_sources)\n",
    "len(all_sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en', '1 (workman,) porter: Lu-girizal,')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sources[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 'You will soak the delicate part (of the cloth) in beer,'),\n",
       " ('en', '1 (workman,) porter: Lu-girizal,'),\n",
       " ('en', 'account of labor of worktroops;'),\n",
       " ('en', 'and Enlil'),\n",
       " ('akk', 'szum-ma _dumu-mesz_')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sources[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146812/146812 [00:00<00:00, 1611653.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/fak/Projects/CuneiformTranslators/tools/tokenizer_training_data.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_txt_path = os.path.abspath(\"tokenizer_training_data.txt\")\n",
    "with open(tokenizer_txt_path, \"wb\") as f:\n",
    "    for lang, line in tqdm(all_sources):\n",
    "        f.write(bytes(line, \"utf8\"))\n",
    "        f.write(b'\\n')\n",
    "tokenizer_txt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ur-Nanše, king of Lagaš, son of GuniDU, “son” of Gursar, had the temple of Nanše built. The shrine Girsu he built. The Great Oval he built. The E-PA he built. The temple of Gatumdu he built. The Edam he built. The temple of Ninmar he built. The Abzu of the Levee he built.\r\n",
      "Ur-Nanše, king of Lagaš, son of GuniDU, “son” of Gursar, had the temple of Nanše built. (A statue of) Nanše he created. The A-Sanga (canal) he dug, (and) for Nanše into the Sanga he made water enter. (A statue of) Eš-ir he created. Ur-nimin, as the spouse of Nanše he chose by kid-omen. A-edin he built, Ningar he built, E-PA he built, the wall of Lagaš he built. (A statue of) Lugal-iri he created. He had boats of Dilmun from the mountains produce loads of timber.\r\n",
      "ki-tusz ne-ha tusz-u3-da\r\n",
      "mu# kara2-har#{ki#} ba#-hul#\r\n",
      "If (there is) a mole is on . . .\r\n",
      "that of Sîn-muballiț\r\n",
      "{gesz}tukul-ga2 mu-bi sig-sze3 mu-un-gal2\r\n",
      "4 barig of grain per (gin2) I sold.\r\n",
      "ur-bar-ra-gin7\r\n",
      "At that time there should be seven for him, there should be seven for him\r\n"
     ]
    }
   ],
   "source": [
    "!tail tokenizer_training_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " '[...]',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_1>',\n",
       " '<extra_id_2>',\n",
       " '<extra_id_3>',\n",
       " '<extra_id_4>',\n",
       " '<extra_id_5>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens=[\"<pad>\", \"</s>\", \"<unk>\", \"[...]\"]\n",
    "additional_special_tokens = [f\"<extra_id_{i}>\" for i in range(100)]\n",
    "all_special_tokens = special_tokens + additional_special_tokens\n",
    "all_special_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "trainer = BpeTrainer(vocab_size=max_vocab_size, special_tokens=all_special_tokens)\n",
    "\n",
    "# print(tokenizer.pre_tokenizer)\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False) # Split(\"\\n\", \"removed\")\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"$0 </s>\",\n",
    "    pair=\"$A </s> $B:1 </s>:1\",\n",
    "    special_tokens=[(x, i) for i, x in enumerate(all_special_tokens)],\n",
    ")\n",
    "# tokenizer.post_processor = tokenizers.processors. tokenizers.processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "tokenizer.model_max_length=model_max_length\n",
    "files = [tokenizer_txt_path]\n",
    "tokenizer.train(files, trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38543"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its work: 26 2/3 (volume-)sar;\n",
      "[759, 801, 129, 4301, 390, 118, 122, 314, 3464, 2511, 686, 130, 1]\n",
      "its work: 26 2/3 (volume-)sar;\n"
     ]
    }
   ],
   "source": [
    "test_txt = all_sources[6][1]\n",
    "test_tokens = tokenizer.encode(test_txt).ids\n",
    "print(test_txt)\n",
    "print(test_tokens)\n",
    "print(tokenizer.decode(ids=test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(tokenizer.encode(\"Hello, my name is Frank\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, model_max_len=model_max_length)\n",
    "ptokenizer.model_max_length = model_max_length\n",
    "ptokenizer.pad_token = \"<pad>\"\n",
    "ptokenizer.pad_token_id = tokenizer.encode(\"<pad>\").ids[0]\n",
    "ptokenizer.eos_token = \"</s>\"\n",
    "ptokenizer.eos_token_id = tokenizer.encode(\"</s>\").ids[0]\n",
    "ptokenizer.unk_token = \"<unk>\"\n",
    "ptokenizer.unk_token_id = tokenizer.encode(\"<unk>\").ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Frank</s>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptokenizer.decode(ptokenizer.encode(\"Hello, my name is Frank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ptokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources_dataset = Dataset.from_dict({\"source\": [x[1] for x in all_sources[:1000]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'ka da-mi-ma-ma-sze3 ma2 gid2-da'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sources_dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = all_sources_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tests = dataset[\"test\"]\n",
    "original_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad <pad> 0\n",
      "eos </s> 1\n",
      "unk <unk> 2\n"
     ]
    }
   ],
   "source": [
    "print(\"pad\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"eos\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(\"unk\", tokenizer.unk_token, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_sources(sources):\n",
    "    \n",
    "    nsources = len(sources)\n",
    "    print(\"-\"*10, nsources)\n",
    "    targets = []\n",
    "    for i in range(nsources):\n",
    "        print(len(sources[i]))\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2bf3887c3f4419852f126f0a4fb82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 900\n",
      "190\n",
      "9\n",
      "4\n",
      "17\n",
      "19\n",
      "6\n",
      "11\n",
      "6\n",
      "15\n",
      "2\n",
      "10\n",
      "7\n",
      "25\n",
      "10\n",
      "393\n",
      "6\n",
      "20\n",
      "15\n",
      "11\n",
      "79\n",
      "5\n",
      "12\n",
      "13\n",
      "11\n",
      "14\n",
      "12\n",
      "21\n",
      "13\n",
      "9\n",
      "4\n",
      "9\n",
      "5\n",
      "19\n",
      "18\n",
      "4\n",
      "9\n",
      "17\n",
      "28\n",
      "15\n",
      "13\n",
      "42\n",
      "11\n",
      "41\n",
      "21\n",
      "5\n",
      "14\n",
      "33\n",
      "18\n",
      "15\n",
      "12\n",
      "10\n",
      "13\n",
      "16\n",
      "8\n",
      "3\n",
      "9\n",
      "6\n",
      "8\n",
      "5\n",
      "6\n",
      "13\n",
      "18\n",
      "33\n",
      "17\n",
      "13\n",
      "8\n",
      "29\n",
      "10\n",
      "31\n",
      "2\n",
      "37\n",
      "14\n",
      "14\n",
      "17\n",
      "6\n",
      "8\n",
      "8\n",
      "45\n",
      "10\n",
      "5\n",
      "8\n",
      "35\n",
      "6\n",
      "104\n",
      "7\n",
      "19\n",
      "7\n",
      "5\n",
      "458\n",
      "12\n",
      "2\n",
      "22\n",
      "16\n",
      "9\n",
      "13\n",
      "20\n",
      "7\n",
      "10\n",
      "401\n",
      "18\n",
      "39\n",
      "10\n",
      "5\n",
      "21\n",
      "7\n",
      "14\n",
      "15\n",
      "26\n",
      "7\n",
      "12\n",
      "12\n",
      "5\n",
      "50\n",
      "6\n",
      "5\n",
      "3\n",
      "208\n",
      "8\n",
      "5\n",
      "6\n",
      "12\n",
      "5\n",
      "12\n",
      "12\n",
      "16\n",
      "11\n",
      "13\n",
      "12\n",
      "15\n",
      "24\n",
      "305\n",
      "3\n",
      "6\n",
      "11\n",
      "7\n",
      "6\n",
      "33\n",
      "11\n",
      "7\n",
      "22\n",
      "23\n",
      "8\n",
      "10\n",
      "30\n",
      "5\n",
      "5\n",
      "27\n",
      "21\n",
      "6\n",
      "165\n",
      "34\n",
      "18\n",
      "19\n",
      "5\n",
      "8\n",
      "7\n",
      "6\n",
      "6\n",
      "7\n",
      "15\n",
      "11\n",
      "17\n",
      "5\n",
      "3\n",
      "3\n",
      "19\n",
      "363\n",
      "49\n",
      "13\n",
      "6\n",
      "84\n",
      "26\n",
      "17\n",
      "22\n",
      "15\n",
      "15\n",
      "7\n",
      "76\n",
      "3\n",
      "10\n",
      "6\n",
      "11\n",
      "21\n",
      "88\n",
      "11\n",
      "2\n",
      "30\n",
      "9\n",
      "10\n",
      "27\n",
      "10\n",
      "12\n",
      "7\n",
      "23\n",
      "4\n",
      "5\n",
      "42\n",
      "8\n",
      "6\n",
      "25\n",
      "12\n",
      "28\n",
      "9\n",
      "6\n",
      "8\n",
      "7\n",
      "19\n",
      "11\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "18\n",
      "4\n",
      "25\n",
      "33\n",
      "7\n",
      "6\n",
      "20\n",
      "4\n",
      "18\n",
      "35\n",
      "8\n",
      "14\n",
      "14\n",
      "4\n",
      "9\n",
      "11\n",
      "13\n",
      "61\n",
      "10\n",
      "3\n",
      "16\n",
      "8\n",
      "8\n",
      "12\n",
      "9\n",
      "31\n",
      "10\n",
      "7\n",
      "5\n",
      "26\n",
      "20\n",
      "11\n",
      "28\n",
      "5\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "18\n",
      "8\n",
      "21\n",
      "26\n",
      "9\n",
      "11\n",
      "185\n",
      "23\n",
      "9\n",
      "3\n",
      "15\n",
      "241\n",
      "17\n",
      "18\n",
      "4\n",
      "4\n",
      "16\n",
      "25\n",
      "18\n",
      "7\n",
      "3\n",
      "9\n",
      "10\n",
      "111\n",
      "9\n",
      "22\n",
      "18\n",
      "125\n",
      "19\n",
      "17\n",
      "6\n",
      "35\n",
      "17\n",
      "19\n",
      "4\n",
      "10\n",
      "8\n",
      "5\n",
      "11\n",
      "40\n",
      "14\n",
      "10\n",
      "22\n",
      "8\n",
      "7\n",
      "23\n",
      "18\n",
      "6\n",
      "7\n",
      "33\n",
      "13\n",
      "8\n",
      "15\n",
      "7\n",
      "50\n",
      "4\n",
      "6\n",
      "18\n",
      "7\n",
      "9\n",
      "16\n",
      "9\n",
      "14\n",
      "68\n",
      "3\n",
      "9\n",
      "9\n",
      "19\n",
      "5\n",
      "14\n",
      "10\n",
      "13\n",
      "22\n",
      "14\n",
      "13\n",
      "9\n",
      "10\n",
      "10\n",
      "4\n",
      "8\n",
      "6\n",
      "4\n",
      "15\n",
      "6\n",
      "35\n",
      "5\n",
      "11\n",
      "13\n",
      "11\n",
      "12\n",
      "9\n",
      "8\n",
      "4\n",
      "16\n",
      "7\n",
      "4\n",
      "20\n",
      "18\n",
      "17\n",
      "5\n",
      "5\n",
      "29\n",
      "10\n",
      "17\n",
      "21\n",
      "16\n",
      "8\n",
      "10\n",
      "14\n",
      "16\n",
      "32\n",
      "13\n",
      "17\n",
      "9\n",
      "4\n",
      "11\n",
      "6\n",
      "8\n",
      "11\n",
      "9\n",
      "2\n",
      "20\n",
      "7\n",
      "6\n",
      "10\n",
      "10\n",
      "29\n",
      "5\n",
      "21\n",
      "27\n",
      "17\n",
      "12\n",
      "13\n",
      "10\n",
      "7\n",
      "6\n",
      "16\n",
      "11\n",
      "7\n",
      "7\n",
      "27\n",
      "18\n",
      "14\n",
      "11\n",
      "4\n",
      "3\n",
      "8\n",
      "26\n",
      "37\n",
      "16\n",
      "24\n",
      "4\n",
      "5\n",
      "16\n",
      "25\n",
      "4\n",
      "3\n",
      "21\n",
      "24\n",
      "144\n",
      "7\n",
      "23\n",
      "12\n",
      "12\n",
      "28\n",
      "431\n",
      "3\n",
      "18\n",
      "13\n",
      "14\n",
      "9\n",
      "9\n",
      "13\n",
      "8\n",
      "14\n",
      "8\n",
      "17\n",
      "28\n",
      "21\n",
      "8\n",
      "16\n",
      "9\n",
      "3\n",
      "13\n",
      "7\n",
      "12\n",
      "300\n",
      "13\n",
      "22\n",
      "8\n",
      "32\n",
      "6\n",
      "4\n",
      "21\n",
      "4\n",
      "9\n",
      "9\n",
      "19\n",
      "4\n",
      "10\n",
      "31\n",
      "11\n",
      "32\n",
      "13\n",
      "6\n",
      "13\n",
      "21\n",
      "5\n",
      "8\n",
      "26\n",
      "4\n",
      "11\n",
      "6\n",
      "4\n",
      "10\n",
      "20\n",
      "20\n",
      "9\n",
      "18\n",
      "23\n",
      "12\n",
      "17\n",
      "18\n",
      "7\n",
      "29\n",
      "13\n",
      "11\n",
      "112\n",
      "9\n",
      "12\n",
      "18\n",
      "7\n",
      "72\n",
      "8\n",
      "20\n",
      "20\n",
      "28\n",
      "15\n",
      "33\n",
      "6\n",
      "165\n",
      "14\n",
      "20\n",
      "11\n",
      "6\n",
      "4\n",
      "204\n",
      "15\n",
      "8\n",
      "5\n",
      "21\n",
      "12\n",
      "8\n",
      "23\n",
      "4\n",
      "21\n",
      "25\n",
      "11\n",
      "9\n",
      "14\n",
      "12\n",
      "4\n",
      "17\n",
      "9\n",
      "8\n",
      "10\n",
      "19\n",
      "57\n",
      "16\n",
      "35\n",
      "22\n",
      "9\n",
      "150\n",
      "15\n",
      "3\n",
      "16\n",
      "6\n",
      "23\n",
      "68\n",
      "17\n",
      "6\n",
      "14\n",
      "9\n",
      "33\n",
      "5\n",
      "41\n",
      "20\n",
      "15\n",
      "36\n",
      "2\n",
      "164\n",
      "53\n",
      "422\n",
      "10\n",
      "11\n",
      "6\n",
      "6\n",
      "41\n",
      "14\n",
      "16\n",
      "17\n",
      "21\n",
      "11\n",
      "17\n",
      "11\n",
      "16\n",
      "18\n",
      "6\n",
      "24\n",
      "11\n",
      "12\n",
      "7\n",
      "18\n",
      "43\n",
      "7\n",
      "25\n",
      "10\n",
      "11\n",
      "6\n",
      "8\n",
      "17\n",
      "16\n",
      "6\n",
      "6\n",
      "11\n",
      "12\n",
      "6\n",
      "12\n",
      "12\n",
      "10\n",
      "12\n",
      "13\n",
      "274\n",
      "8\n",
      "25\n",
      "7\n",
      "8\n",
      "16\n",
      "4\n",
      "14\n",
      "16\n",
      "14\n",
      "432\n",
      "5\n",
      "25\n",
      "8\n",
      "52\n",
      "22\n",
      "4\n",
      "7\n",
      "7\n",
      "9\n",
      "23\n",
      "8\n",
      "25\n",
      "35\n",
      "13\n",
      "15\n",
      "6\n",
      "17\n",
      "13\n",
      "153\n",
      "46\n",
      "25\n",
      "14\n",
      "8\n",
      "5\n",
      "13\n",
      "34\n",
      "7\n",
      "56\n",
      "8\n",
      "6\n",
      "7\n",
      "25\n",
      "7\n",
      "22\n",
      "12\n",
      "10\n",
      "7\n",
      "14\n",
      "10\n",
      "3\n",
      "4\n",
      "22\n",
      "15\n",
      "19\n",
      "11\n",
      "24\n",
      "11\n",
      "5\n",
      "11\n",
      "17\n",
      "3\n",
      "8\n",
      "92\n",
      "16\n",
      "4\n",
      "37\n",
      "31\n",
      "46\n",
      "22\n",
      "3\n",
      "5\n",
      "6\n",
      "32\n",
      "6\n",
      "8\n",
      "29\n",
      "17\n",
      "5\n",
      "12\n",
      "12\n",
      "19\n",
      "3\n",
      "11\n",
      "18\n",
      "10\n",
      "12\n",
      "26\n",
      "20\n",
      "10\n",
      "66\n",
      "17\n",
      "14\n",
      "31\n",
      "7\n",
      "9\n",
      "11\n",
      "14\n",
      "17\n",
      "32\n",
      "2\n",
      "8\n",
      "29\n",
      "31\n",
      "6\n",
      "14\n",
      "14\n",
      "217\n",
      "14\n",
      "8\n",
      "17\n",
      "9\n",
      "10\n",
      "59\n",
      "12\n",
      "11\n",
      "9\n",
      "49\n",
      "42\n",
      "6\n",
      "6\n",
      "35\n",
      "15\n",
      "73\n",
      "69\n",
      "4\n",
      "12\n",
      "9\n",
      "16\n",
      "10\n",
      "10\n",
      "8\n",
      "20\n",
      "10\n",
      "16\n",
      "228\n",
      "13\n",
      "5\n",
      "9\n",
      "7\n",
      "8\n",
      "16\n",
      "18\n",
      "17\n",
      "10\n",
      "23\n",
      "8\n",
      "3\n",
      "36\n",
      "14\n",
      "23\n",
      "23\n",
      "22\n",
      "13\n",
      "27\n",
      "19\n",
      "14\n",
      "14\n",
      "4\n",
      "51\n",
      "14\n",
      "5\n",
      "7\n",
      "5\n",
      "21\n",
      "20\n",
      "38\n",
      "34\n",
      "28\n",
      "18\n",
      "4\n",
      "3\n",
      "7\n",
      "18\n",
      "15\n",
      "26\n",
      "13\n",
      "10\n",
      "28\n",
      "10\n",
      "6\n",
      "4\n",
      "6\n",
      "10\n",
      "10\n",
      "216\n",
      "28\n",
      "58\n",
      "7\n",
      "55\n",
      "9\n",
      "4\n",
      "14\n",
      "12\n",
      "9\n",
      "7\n",
      "18\n",
      "5\n",
      "5\n",
      "17\n",
      "5\n",
      "5\n",
      "6\n",
      "32\n",
      "9\n",
      "7\n",
      "19\n",
      "24\n",
      "7\n",
      "15\n",
      "15\n",
      "33\n",
      "189\n",
      "9\n",
      "9\n",
      "96\n",
      "10\n",
      "5\n",
      "10\n",
      "11\n",
      "15\n",
      "5\n",
      "14\n",
      "37\n",
      "18\n",
      "7\n",
      "121\n",
      "15\n",
      "21\n",
      "22\n",
      "7\n",
      "4\n",
      "24\n",
      "7\n",
      "4\n",
      "13\n",
      "21\n",
      "6\n",
      "20\n",
      "11\n",
      "10\n",
      "3\n",
      "23\n",
      "4\n",
      "12\n",
      "10\n",
      "14\n",
      "8\n",
      "19\n",
      "15\n",
      "9\n",
      "10\n",
      "12\n",
      "18\n",
      "13\n",
      "10\n",
      "34\n",
      "5\n",
      "18\n",
      "3\n",
      "20\n",
      "5\n",
      "10\n",
      "9\n",
      "30\n",
      "10\n",
      "11\n",
      "2\n",
      "5\n",
      "9\n",
      "14\n",
      "11\n",
      "10\n",
      "44\n",
      "6\n",
      "15\n",
      "10\n",
      "7\n",
      "18\n",
      "16\n",
      "9\n",
      "10\n",
      "133\n",
      "9\n",
      "7\n",
      "10\n",
      "12\n",
      "14\n",
      "17\n",
      "6\n",
      "10\n",
      "34\n",
      "10\n",
      "9\n",
      "28\n",
      "16\n",
      "11\n",
      "10\n",
      "27\n",
      "11\n",
      "12\n",
      "11\n",
      "13\n",
      "26\n",
      "19\n",
      "112\n",
      "14\n",
      "13\n",
      "6\n",
      "6\n",
      "23\n",
      "28\n",
      "7\n",
      "8\n",
      "6\n",
      "23\n",
      "13\n",
      "7\n",
      "[165, 703, 165, 298, 169, 194, 573, 116, 277, 116, 361, 116, 282, 316, 354, 121, 165, 272, 301, 297, 301, 507, 106, 3534, 170, 121, 2672, 316, 703, 165, 1780, 116, 531, 121, 405, 169, 194, 270, 116, 321, 116, 285, 316, 174, 122, 116, 320, 127, 589, 121, 418, 121, 116, 340, 123, 116, 166, 165, 316, 703, 165, 298, 169, 194, 1440, 116, 335, 116, 309, 116, 387, 121, 116, 308, 115, 316, 174, 122, 116, 320, 127, 165, 316, 703, 165, 272, 116, 309, 116, 403, 116, 452, 116, 646, 316, 174, 122, 116, 320, 127, 165, 316, 994, 122, 165, 298, 169, 194, 1151, 116, 335, 116, 384, 116, 1068, 316, 346, 116, 174, 165, 316, 628, 537, 116, 170, 122, 293, 123, 328, 111, 186, 112, 637, 111, 322, 438, 542, 165, 316, 335, 567, 116, 524, 116, 295, 116, 315, 116, 270, 460, 116, 170, 434, 982, 121, 429, 116, 274, 369, 116, 552, 121, 298, 169, 194, 643, 229, 3, 428, 124, 301, 296, 348, 116, 310, 116, 264, 116, 282, 106, 537, 116, 479, 116, 479, 330, 229, 3, 165, 1]\n",
      "[165, 703, 165, 298, 169, 194, 573, 116, 277, 116, 361, 116, 282, 316, 354, 121, 165, 272, 301, 297, 301, 507, 106, 3534, 170, 121, 2672, 316, 703, 165, 1780, 116, 531, 121, 405, 169, 194, 270, 116, 321, 116, 285, 316, 174, 122, 116, 320, 127, 589, 121, 418, 121, 116, 340, 123, 116, 166, 165, 316, 703, 165, 298, 169, 194, 1440, 116, 335, 116, 309, 116, 387, 121, 116, 308, 115, 316, 174, 122, 116, 320, 127, 165, 316, 703, 165, 272, 116, 309, 116, 403, 116, 452, 116, 646, 316, 174, 122, 116, 320, 127, 165, 316, 994, 122, 165, 298, 169, 194, 1151, 116, 335, 116, 384, 116, 1068, 316, 346, 116, 174, 165, 316, 628, 537, 116, 170, 122, 293, 123, 328, 111, 186, 112, 637, 111, 322, 438, 542, 165, 316, 335, 567, 116, 524, 116, 295, 116, 315, 116, 270, 460, 116, 170, 434, 982, 121, 429, 116, 274, 369, 116, 552, 121, 298, 169, 194, 643, 229, 3, 428, 124, 301, 296, 348, 116, 310, 116, 264, 116, 282, 106, 537, 116, 479, 116, 479, 330, 229, 3, 165, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1643831f289455eb14353f1fab6d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 100\n",
      "9\n",
      "8\n",
      "4\n",
      "8\n",
      "13\n",
      "4\n",
      "9\n",
      "12\n",
      "13\n",
      "30\n",
      "7\n",
      "31\n",
      "5\n",
      "30\n",
      "13\n",
      "5\n",
      "10\n",
      "10\n",
      "35\n",
      "5\n",
      "5\n",
      "12\n",
      "30\n",
      "16\n",
      "23\n",
      "7\n",
      "26\n",
      "17\n",
      "15\n",
      "12\n",
      "5\n",
      "14\n",
      "14\n",
      "5\n",
      "14\n",
      "8\n",
      "13\n",
      "6\n",
      "8\n",
      "6\n",
      "6\n",
      "11\n",
      "16\n",
      "21\n",
      "10\n",
      "6\n",
      "7\n",
      "13\n",
      "21\n",
      "12\n",
      "23\n",
      "170\n",
      "12\n",
      "9\n",
      "10\n",
      "6\n",
      "413\n",
      "5\n",
      "11\n",
      "11\n",
      "23\n",
      "9\n",
      "13\n",
      "24\n",
      "6\n",
      "5\n",
      "128\n",
      "29\n",
      "13\n",
      "12\n",
      "10\n",
      "11\n",
      "8\n",
      "17\n",
      "22\n",
      "4\n",
      "13\n",
      "13\n",
      "12\n",
      "4\n",
      "4\n",
      "16\n",
      "29\n",
      "15\n",
      "65\n",
      "13\n",
      "6\n",
      "7\n",
      "133\n",
      "6\n",
      "41\n",
      "29\n",
      "4\n",
      "17\n",
      "4\n",
      "27\n",
      "13\n",
      "6\n",
      "51\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccc = 0\n",
    "sum_src_chars_per_token = 0.0\n",
    "num_src_chars_per_token = 0\n",
    "sum_tgt_chars_per_token = 0.0\n",
    "num_tgt_chars_per_token = 0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global ccc, sum_src_chars_per_token, sum_tgt_chars_per_token, num_src_chars_per_token, num_tgt_chars_per_token\n",
    "#     print(examples)\n",
    "    inputs = [example for example in examples[\"source\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=model_max_length, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(targets, max_length=model_max_length, truncation=True)\n",
    "        labels = corrupt_sources(model_inputs[\"input_ids\"])\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    nexamples = len(inputs)\n",
    "    for i in range(nexamples):\n",
    "        nchar = len(inputs[i])\n",
    "        ntoks = len(model_inputs[\"input_ids\"][i])\n",
    "        if ntoks > 0:\n",
    "            sum_src_chars_per_token += nchar / ntoks\n",
    "            num_src_chars_per_token += 1\n",
    "    \n",
    "    ccc += 1\n",
    "    if ccc == 1:\n",
    "        print(model_inputs[\"input_ids\"][0])\n",
    "        print(model_inputs[\"labels\"][0])\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_src_chars_per_token = 2.70826004394177\n"
     ]
    }
   ],
   "source": [
    "avg_src_chars_per_token = sum_src_chars_per_token / num_src_chars_per_token\n",
    "print(\"avg_src_chars_per_token\", \"=\", avg_src_chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].remove_columns([\"source\"])\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].remove_columns([\"source\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_max_length = max([len(x[\"input_ids\"]) for x in tokenized_dataset[\"train\"]])\n",
    "source_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[165,\n",
       " 703,\n",
       " 165,\n",
       " 298,\n",
       " 169,\n",
       " 194,\n",
       " 573,\n",
       " 116,\n",
       " 277,\n",
       " 116,\n",
       " 361,\n",
       " 116,\n",
       " 282,\n",
       " 316,\n",
       " 354,\n",
       " 121,\n",
       " 165,\n",
       " 272,\n",
       " 301,\n",
       " 297,\n",
       " 301,\n",
       " 507,\n",
       " 106,\n",
       " 3534,\n",
       " 170,\n",
       " 121,\n",
       " 2672,\n",
       " 316,\n",
       " 703,\n",
       " 165,\n",
       " 1780,\n",
       " 116,\n",
       " 531,\n",
       " 121,\n",
       " 405,\n",
       " 169,\n",
       " 194,\n",
       " 270,\n",
       " 116,\n",
       " 321,\n",
       " 116,\n",
       " 285,\n",
       " 316,\n",
       " 174,\n",
       " 122,\n",
       " 116,\n",
       " 320,\n",
       " 127,\n",
       " 589,\n",
       " 121,\n",
       " 418,\n",
       " 121,\n",
       " 116,\n",
       " 340,\n",
       " 123,\n",
       " 116,\n",
       " 166,\n",
       " 165,\n",
       " 316,\n",
       " 703,\n",
       " 165,\n",
       " 298,\n",
       " 169,\n",
       " 194,\n",
       " 1440,\n",
       " 116,\n",
       " 335,\n",
       " 116,\n",
       " 309,\n",
       " 116,\n",
       " 387,\n",
       " 121,\n",
       " 116,\n",
       " 308,\n",
       " 115,\n",
       " 316,\n",
       " 174,\n",
       " 122,\n",
       " 116,\n",
       " 320,\n",
       " 127,\n",
       " 165,\n",
       " 316,\n",
       " 703,\n",
       " 165,\n",
       " 272,\n",
       " 116,\n",
       " 309,\n",
       " 116,\n",
       " 403,\n",
       " 116,\n",
       " 452,\n",
       " 116,\n",
       " 646,\n",
       " 316,\n",
       " 174,\n",
       " 122,\n",
       " 116,\n",
       " 320,\n",
       " 127,\n",
       " 165,\n",
       " 316,\n",
       " 994,\n",
       " 122,\n",
       " 165,\n",
       " 298,\n",
       " 169,\n",
       " 194,\n",
       " 1151,\n",
       " 116,\n",
       " 335,\n",
       " 116,\n",
       " 384,\n",
       " 116,\n",
       " 1068,\n",
       " 316,\n",
       " 346,\n",
       " 116,\n",
       " 174,\n",
       " 165,\n",
       " 316,\n",
       " 628,\n",
       " 537,\n",
       " 116,\n",
       " 170,\n",
       " 122,\n",
       " 293,\n",
       " 123,\n",
       " 328,\n",
       " 111,\n",
       " 186,\n",
       " 112,\n",
       " 637,\n",
       " 111,\n",
       " 322,\n",
       " 438,\n",
       " 542,\n",
       " 165,\n",
       " 316,\n",
       " 335,\n",
       " 567,\n",
       " 116,\n",
       " 524,\n",
       " 116,\n",
       " 295,\n",
       " 116,\n",
       " 315,\n",
       " 116,\n",
       " 270,\n",
       " 460,\n",
       " 116,\n",
       " 170,\n",
       " 434,\n",
       " 982,\n",
       " 121,\n",
       " 429,\n",
       " 116,\n",
       " 274,\n",
       " 369,\n",
       " 116,\n",
       " 552,\n",
       " 121,\n",
       " 298,\n",
       " 169,\n",
       " 194,\n",
       " 643,\n",
       " 229,\n",
       " 3,\n",
       " 428,\n",
       " 124,\n",
       " 301,\n",
       " 296,\n",
       " 348,\n",
       " 116,\n",
       " 310,\n",
       " 116,\n",
       " 264,\n",
       " 116,\n",
       " 282,\n",
       " 106,\n",
       " 537,\n",
       " 116,\n",
       " 479,\n",
       " 116,\n",
       " 479,\n",
       " 330,\n",
       " 229,\n",
       " 3,\n",
       " 165,\n",
       " 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfsdfsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdfsdfsdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdfsdfsdf' is not defined"
     ]
    }
   ],
   "source": [
    "sdfsdfsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id, \n",
    "                                                   max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 38543)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = copy.deepcopy(base_model.config)\n",
    "model_config.vocab_size = tokenizer.vocab_size\n",
    "model_config.max_length, model_config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "# data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132130"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_sequences = len(tokenized_dataset[\"train\"])\n",
    "num_train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../results/{model_id}\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=int(0.5 * num_train_sequences/batch_size),\n",
    "    warmup_steps=warm,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=has_cuda,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/fak/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 132130\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 495510\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpraeclarum\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fak/Projects/CuneiformTranslators/tools/wandb/run-20220726_155143-9af82kwq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/praeclarum/huggingface/runs/9af82kwq\" target=\"_blank\">../results/t5-base-pre-p-l-akksux-en-20220726-154724</a></strong> to <a href=\"https://wandb.ai/praeclarum/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33802' max='495510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 33802/495510 1:09:50 < 15:54:03, 8.07 it/s, Epoch 2.05/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.155500</td>\n",
       "      <td>1.867853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.726700</td>\n",
       "      <td>1.485119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14682\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33000\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33000/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-31500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14682\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33500\n",
      "Configuration saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33500/config.json\n",
      "Model weights saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-pre-p-l-akksux-en-20220726-154724/checkpoint-32000] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1560\u001b[0m ):\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mscale_loss(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;28;01mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TranslationPipeline(model=model.to(\"cpu\"), tokenizer=tokenizer, max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline(\"translate English to French: hello my name is Frank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test = translations[\"test\"][0][\"source\"]\n",
    "target_test = translations[\"test\"][0][\"target\"]\n",
    "print(source_test)\n",
    "print(\"-\"*80)\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    return pipeline(text)\n",
    "\n",
    "translate(source_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests = original_tests\n",
    "def sample(num_samples=100):\n",
    "    for i in range(min(num_samples, tests.num_rows)):\n",
    "        t = tests[i]\n",
    "    #     print(t)\n",
    "        src = t[\"source\"]\n",
    "        tgt = t[\"target\"]\n",
    "        query = src\n",
    "        pred = pipeline(query)[0][\"translation_text\"]\n",
    "        print(\"-\"*48)\n",
    "        print(\"QUERY \", query)\n",
    "        print(\"TARGET\", tgt)\n",
    "        print(\"PRED  \", pred)\n",
    "    #     break\n",
    "    \n",
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.abspath(f\"/home/fak/nn/Data/generated/cuneiform/{model_id}-pretrained\")\n",
    "trainer.save_model(model_path)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
