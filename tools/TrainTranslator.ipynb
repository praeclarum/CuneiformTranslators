{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Translator\n",
    "\n",
    "Based on: https://huggingface.co/docs/transformers/tasks/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, datetime\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import TranslationPipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "import cdli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-240500  checkpoint-241000  checkpoint-241500\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../results/t5-base-p-akksux-en-20220722-173018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetune_model_id(model_id):\n",
    "    model_dir = f\"../results/{model_id}\"\n",
    "    checkpoints = [(os.path.abspath(x), int(os.path.split(x)[1].split(\"-\")[1])) for x in glob.glob(f\"{model_dir}/checkpoint-*\")]\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "#     print(checkpoints)\n",
    "    return checkpoints[0]\n",
    "    \n",
    "# get_finetune_model_id(\"t5-base-p-akksux-en-20220722-173018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"TrainTranslator.ipynb\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "source_langs = set([\"akk\"])\n",
    "\n",
    "# target_langs = set([\"en\", \"it\", \"es\", \"fr\", \"de\"])\n",
    "target_langs = set([\"en\"])\n",
    "\n",
    "base_model_id = \"t5-base\"\n",
    "finetune_model_id = None\n",
    "finetune_model_id = get_finetune_model_id(\"t5-base-p-akksux-en-20220722-173018\")\n",
    "\n",
    "model_max_length = 512\n",
    "batch_size = 8 if os.path.basename(base_model_id).startswith(\"t5-base\") else 128\n",
    "\n",
    "num_train_epochs = 30\n",
    "\n",
    "is_bi = False\n",
    "paragraphs = True\n",
    "is_finetune = finetune_model_id is not None and len(finetune_model_id) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "flags = \"\"\n",
    "suffix = \"\"\n",
    "if is_bi:\n",
    "    flags += \"-bi\"\n",
    "if paragraphs:\n",
    "    flags += \"-p\"\n",
    "if is_finetune:\n",
    "    flags += \"-f\"\n",
    "    suffix += f\"-{os.path.basename(os.path.split(finetune_model_id)[0])}-{os.path.basename(finetune_model_id)}\"\n",
    "model_id = f\"{os.path.basename(base_model_id)}{flags}-{''.join(sorted(list(source_langs)))}-{''.join(sorted(list(target_langs)))}-{date_id}{suffix}\"\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, <torch.cuda.device at 0x7f5eea740580>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.cuda.device(0) if has_cuda else \"cpu\"\n",
    "has_cuda, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 25 17:01:49 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   49C    P8    31W / 350W |    168MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       971      G   /usr/lib/xorg/Xorg                120MiB |\r\n",
      "|    0   N/A  N/A      1236      G   ...ome-remote-desktop-daemon        4MiB |\r\n",
      "|    0   N/A  N/A      1272      G   /usr/bin/gnome-shell               23MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_src_chars_per_token = 1.8713256996006793\n",
    "avg_tgt_chars_per_token = 2.577806274115267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_full = {\n",
    "    \"akk\": \"Akkadian\",\n",
    "    \"elx\": \"Elamite\",\n",
    "    \"sux\": \"Sumerian\",\n",
    "    \"akkts\": \"Akkadian\",\n",
    "    \"elxts\": \"Elamite\",\n",
    "    \"suxts\": \"Sumerian\",\n",
    "    \"en\": \"English\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"fr\": \"French\",\n",
    "    \"de\": \"German\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate Sumerian to Spanish: '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prefix(src_lang, tgt_lang):\n",
    "    s = lang_full[src_lang]\n",
    "    t = lang_full[tgt_lang]\n",
    "    return f\"translate {s} to {t}: \"\n",
    "    \n",
    "get_prefix(\"suxts\", \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"ā\", \"a\"),\n",
    "    (\"Ā\", \"a\"),\n",
    "    (\"ḫ\", \"h\"),\n",
    "    (\"Ḫ\", \"H\"),\n",
    "    (\"ī\", \"i\"),\n",
    "    (\"Ī\", \"I\"),\n",
    "#     (\"î\", \"i\"),\n",
    "#     (\"Î\", \"I\"),\n",
    "    (\"ř\", \"r\"),\n",
    "    (\"Ř\", \"R\"),\n",
    "    (\"š\", \"sh\"),\n",
    "    (\"Š\", \"Sh\"),\n",
    "    (\"ṣ\", \"sh\"),\n",
    "    (\"Ṣ\", \"Sh\"),\n",
    "    (\"ṭ\", \"t\"),\n",
    "    (\"Ṭ\", \"T\"),\n",
    "    (\"ū\", \"u\"),\n",
    "    (\"Ū\", \"U\"),\n",
    "]\n",
    "def replace_unsupported(text):\n",
    "    r = text\n",
    "    for s, t in replacements:\n",
    "        r = r.replace(s, t)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/cdli-gh/data/raw/master/cdliatf_unblocked.atf\n",
      "Parsing atf\n"
     ]
    }
   ],
   "source": [
    "publications = cdli.get_atf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134712, 'publications')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publications), \"publications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False ''\n",
      "False ' '\n",
      "False 'xx xxx x'\n",
      "False '.. . .. '\n",
      "True 'Hi'\n"
     ]
    }
   ],
   "source": [
    "def target_ok(target_text):\n",
    "    if len(target_text) == 0:\n",
    "        return False\n",
    "    if len(set(target_text.replace(\" \", \"\"))) < 2:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "def test_target_ok(text):\n",
    "    ok = target_ok(text)\n",
    "    print(ok, repr(text))\n",
    "    \n",
    "test_target_ok(\"\")\n",
    "test_target_ok(\" \")\n",
    "test_target_ok(\"xx xxx x\")\n",
    "test_target_ok(\".. . .. \")\n",
    "test_target_ok(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing akk to en\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fb1988be734ad09ee32744fd11d636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 18189\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sourceandtargets = []\n",
    "\n",
    "added_sources = set()\n",
    "\n",
    "wmax_num_tokens = model_max_length - 192\n",
    "\n",
    "def wrap_paragraph(paragraph, lines, src_lang, tgt_lang):\n",
    "    ptag, pline_start_index, pline_end_index = paragraph\n",
    "    wline_ranges = []\n",
    "    wline_tok_len = 0.0\n",
    "    \n",
    "    def start_new_line(pline_index):\n",
    "#         print(\"start\", pline_index)\n",
    "        wline_ranges.append((pline_index, pline_index + 1))\n",
    "        \n",
    "    def append_line(pline_index):\n",
    "#         print(\"append\", pline_index)\n",
    "        r = wline_ranges[-1]\n",
    "        if r[1] == pline_index:\n",
    "            wline_ranges[-1] = (r[0], r[1] + 1)\n",
    "        else:\n",
    "            print(f\"Missing line: got {pline_index}, expected {r[1]}: {wline_ranges}\")\n",
    "\n",
    "    for pline_index in range(pline_start_index, pline_end_index):\n",
    "        pline_num_toks = len(lines[pline_index].text) / avg_src_chars_per_token + 1.0\n",
    "        if len(wline_ranges) == 0 or (wline_tok_len + pline_num_toks > wmax_num_tokens):\n",
    "            start_new_line(pline_index)\n",
    "            wline_tok_len = 0.0\n",
    "        else:\n",
    "            append_line(pline_index)\n",
    "        wline_tok_len += pline_num_toks\n",
    "    return wline_ranges\n",
    "\n",
    "def add_line_ranges(area, b, e):\n",
    "#                     print(\"-\"*50)\n",
    "    ls = \" \".join([x.text for x in area.lines[b:e]])\n",
    "    ls = \" \".join(ls.split(\" \"))\n",
    "    prefixed_ls = st_prefix + ls\n",
    "    if prefixed_ls in added_sources:\n",
    "        return\n",
    "    lt = \" \".join([(x.languages[t] if t in x.languages else \"\") for x in area.lines[b:e]])\n",
    "    lt = \" \".join(lt.split(\" \"))\n",
    "    lt = replace_unsupported(lt)\n",
    "    if not target_ok(lt):\n",
    "        return\n",
    "#                     print(ls)\n",
    "#                     print(lt)\n",
    "    added_sources.add(prefixed_ls)\n",
    "    new_sourceandtargets.append((prefixed_ls, lt))\n",
    "    if is_bi:\n",
    "        new_sourceandtargets.append((ts_prefix + lt, ls))\n",
    "\n",
    "\n",
    "\n",
    "for s in source_langs:    \n",
    "    for t in target_langs:\n",
    "        print(\"Preparing\", s, \"to\", t)\n",
    "        st_prefix = get_prefix(s, t)\n",
    "        ts_prefix = get_prefix(t, s)\n",
    "        for pub in tqdm([p for p in publications if p.language==s]):\n",
    "            for area in pub.text_areas:\n",
    "                if not any(x for x in area.lines if t in x.languages):\n",
    "                    continue\n",
    "                if paragraphs:\n",
    "                    paragraphs = area.lines_to_paragraphs(s)\n",
    "                    line_ranges = []                \n",
    "                    for p in paragraphs:                    \n",
    "                        wlines = wrap_paragraph(p, area.lines, s, t)\n",
    "                        line_ranges.extend(wlines)\n",
    "    #                 print(\"=\"*50, len(area.lines))\n",
    "                    for b, e in line_ranges:\n",
    "                        add_line_ranges(area, b, e)\n",
    "                for i, _ in enumerate(area.lines):\n",
    "                    add_line_ranges(area, i, i + 1)\n",
    "\n",
    "random.shuffle(new_sourceandtargets)\n",
    "new_all_translations = Dataset.from_dict({\"source\": [x[0] for x in new_sourceandtargets], \"target\": [x[1] for x in new_sourceandtargets]})\n",
    "new_all_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_translations = new_all_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': ['translate Akkadian to English: {d}ag _uru3_ ku-dur2',\n",
       "  'translate Akkadian to English: [_a_-szu2 sza2 {disz}x]-x-uri3# {disz}en-szu2-nu#',\n",
       "  'translate Akkadian to English: [{(d)}na-ra-am-{d}]suen _lugal#_ [ki-ib-ra]-tim# ar#-ba#-im# i3-nu HAR-sza-ma-at{ki} en-a#-ra#-am u3 _am_ in qab2!(DA)-[la2]-NI# ti-ba-ar# _sa-tu_-im su4-ma u-sa-am-qi2-it-su tam2-si4-il-su ib-ni-ma a-na {d}en-lil2 a-bi2#-su _a mu-ru_ sza _dub#_ su4-a u-sa-sa3*-ku#-ni {d}en-lil2 u3 {d}utu _suhusz_-su li-su2-ha u3 _sze-numun_-su li-il-qu3-tam2',\n",
       "  'translate Akkadian to English: [unu]{ki}',\n",
       "  'translate Akkadian to English: szum-ma i-na _tur3_',\n",
       "  'translate Akkadian to English: li-il-qu3#-ta2#',\n",
       "  'translate Akkadian to English: _u4-mesz_-szu i-gam-ma-ru',\n",
       "  'translate Akkadian to English: 1(disz) _kusz3 gal_-tu4',\n",
       "  'translate Akkadian to English: t,up-pu sza szi2-ma-at a-gu5-a',\n",
       "  'translate Akkadian to English: in 1(disz) _u4_',\n",
       "  'translate Akkadian to English: mi-na-a ni-i-nu sza2 ni-ib-nu-u2 nu-usz-hal-laq-ma',\n",
       "  'translate Akkadian to English: _egir_-szu2 ar-te-di _gu4-mesz_-szu2 _udu_ s,e-ni-szu2 _nig2-szu_-szu2 a-na la ma-ni u2-ter-ra _iri-mesz_-szu2',\n",
       "  'translate Akkadian to English: i-na bu-ul-t,i3-szu',\n",
       "  'translate Akkadian to English: e-tam-ru-u2-ni i-<s,a>-ba-as-si',\n",
       "  'translate Akkadian to English: lu-uh-isz-an',\n",
       "  'translate Akkadian to English: _BE samag_ i-na zi-it-ti _gu2-husz lu2#_ [...]',\n",
       "  'translate Akkadian to English: ina _igi_-it _gaba_-ka _gub-me_-zu-ma _gi6 mu2-mesz u szu2-mesz_',\n",
       "  'translate Akkadian to English: szum-ma _lu2 ku3-babbar_ a-na pa-ni-szu',\n",
       "  'translate Akkadian to English: _sukkal_ elam{ki} u3 szi-masz-gi5',\n",
       "  'translate Akkadian to English: ki-x-e-na-asz2{ki}',\n",
       "  'translate Akkadian to English: _ibila_ a-sza-re-du',\n",
       "  'translate Akkadian to English: u3 szum-ma _ARAD2 e2-gal_ u3 lu _ARAD2 |MASZ.EN.GAG|_ _dumu-munus_ a-wi-lim i-hu-uz-ma i-nu-ma i-hu-zu-szi qa2-du-um sze-ri-ik-tim sza _e2_ a-bi-sza a-na _e2 ARAD2 e2-gal_ u3 lu _ARAD2 |MASZ.EN.GAG|_ i-ru-ub-ma isz-tu in-ne-em-du _e2_ i-pu-szu bi-sza-am ir-szu-u2 wa-ar-ka-nu-um-ma lu _ARAD2 e2-gal_ u3 lu _ARAD2 |MASZ.EN.GAG|_ a-na szi-im-tim it-ta-la-ak _dumu-munus_ a-wi-lim sze-ri-ik-ta-sza i-le-qe2 u3 mi-im-ma sza mu-sa3 u3 szi-i isz-tu in-ne-em-du ir-szu-u2 a-na szi-ni-szu i-zu-uz-zu-ma mi-isz-lam be-el _ARAD2_ i-le-qe2 mi-isz-lam _dumu-munus_ a-wi-lim',\n",
       "  'translate Akkadian to English: _BE samag_ i-na _gu2-hasz#_ [...]',\n",
       "  'translate Akkadian to English: _u4_-mu ri-du-ti {d}isz-tar ne2-me-li ta-at-tur-ru ik-ri-bi _lugal_ szi-i hi-du-ti u3 ni-gu-ta-szu2 a-na da-me-eq-ti szum-ma u2-sza2-ri a-na _kur_-ia me-e _dingir_ na-s,a-ri szu-mi {d}isz-ta-ri szu-qu-ru _ug3-mesz_-ia usz-ta-hi-iz ta-na-da-a-ti _lugal_ i-lisz u2-masz-szil u3 pu-luh-ti _e2-gal_ um-man u2-szal-mid lu-u i-de ki-i it-ti _dingir_ i-ta-am-gur an-na-a-ti sza dam-qat ra-ma-nu-usz a-na _dingir_ gul-lul-tum sza ina lib3-bi-szu2 mu-us-su-kat3 _ugu dingir_-szu2 dam-qat a-a-u2 t,e3-em _dingir-mesz_ qe2-reb _an_-e i-lam-mad mi-lik sza2 an-za-nun-ze-e i-ha-ak-kim man-nu',\n",
       "  'translate Akkadian to English: u3 ru-gu-um-ma-na-a',\n",
       "  'translate Akkadian to English: _sze_ a-na be-el hu-bu-ul-li-szu',\n",
       "  'translate Akkadian to English: me-szu na-ah-du-tim',\n",
       "  'translate Akkadian to English: i-hi-at 5(iku) _asza5{a-sza3} saga_',\n",
       "  'translate Akkadian to English: mi-im-mu-szu',\n",
       "  'translate Akkadian to English: enkum',\n",
       "  'translate Akkadian to English: {d}ag-nig2-du-uri3 _lugal_ babilax(|DIN.TIR|){ki}',\n",
       "  'translate Akkadian to English: sza a-di larsa{ki}-ma',\n",
       "  'translate Akkadian to English: ib-ba-ab-lu-szum',\n",
       "  'translate Akkadian to English: _1(disz) e2_-tam _lu2_ eb-bi sza it-ti-szu',\n",
       "  'translate Akkadian to English: {iti}bara2 _u4 6(disz)-kam2',\n",
       "  'translate Akkadian to English: _a2 {lu2}tug2-du8-a_',\n",
       "  'translate Akkadian to English: i-bi-ra-am a-lik pa-ni s,a-bi-im sza-tu {disz}mu-tu-ad-gi5-im {disz}hu-la-lum a-na al-la-ha-da{ki} a-na szar-ru-tim pu-uh a-tam-ri-im i-re-ed-du-u2 a-nu-um-ma t,e4-ma-am sza esz-mu-u2 a-na s,e-ri-ka asz-tap-ra-am t,e4-em-ka s,a-ba-at u3 t,e4-ma-am an-ne2-em sza asz-pu-ra-kum ar-hi-isz a-na s,e-er _lugal_ li-ik-szu-ud a#-na# x-x-zi-im a-na [...] sze x [...] [ki-ma] ta#-ki-it-tam i-[...]',\n",
       "  'translate Akkadian to English: szum-ma _munus-kurun-na_ sa3-ar-ru-tum i-na _e2_-sza it-tar-ka-su2-ma sa3-ar-ru-tim szu-nu-ti la is,-s,a-ab-tam-ma a-na _e2-gal_ la ir-de-a-am _munus-kurun-na_ szi-i id-da-ak',\n",
       "  'translate Akkadian to English: i-di3-{d}i7',\n",
       "  'translate Akkadian to English: u3 lu _ARAD2_',\n",
       "  'translate Akkadian to English: a-na pa-ni-szu-nu u2-pa-s,a-an-szi',\n",
       "  'translate Akkadian to English: i+na te-be2-szu',\n",
       "  'translate Akkadian to English: {d}nanna _lugal an ki_ at-ta',\n",
       "  'translate Akkadian to English: er-re-tim',\n",
       "  'translate Akkadian to English: ina _ka2_ ka-lak-ka i-nam-din {lu2}mu-kin-nu {disz}{d}u-gur-asz-suh3-sur _a_-szu2 sza2 {disz}tin-su {disz}{d}be-mu-du3 _a_-szu2 sza2 {disz}nig2-du {disz}si-lim-dingir _a_-szu2 sza2 {disz}la-ba-szi u _{lu2}szid_ {disz}{d}ag-gub-a _a_-szu2 sza2 {disz}{d}marduk-kam2 _a_ {lu2}sipa-gu4 larsa{ki} {iti}bara2 _u4 4(disz)-kam2_ _mu# 3(u) 9(disz)-kam2_ {d}ag-nig2-du-uri3 _lugal_',\n",
       "  'translate Akkadian to English: i#-[sza-da]-ad-ma',\n",
       "  'translate Akkadian to English: u2-ki-in-nam',\n",
       "  'translate Akkadian to English: um-ma _ARAD-mesz_-ka-ma',\n",
       "  'translate Akkadian to English: _lu2 {munus}dam_-su hi-i-t,a',\n",
       "  'translate Akkadian to English: _lu2_-lam',\n",
       "  'translate Akkadian to English: _igi_ {d}iszkur-ma-an-szum2 ra-bi si-ik-ka3#-tum',\n",
       "  'translate Akkadian to English: sza-a-a-ma-nu-ma',\n",
       "  'translate Akkadian to English: [x x x x?] il-ta#-ka-an x-x-x [... i-na] _i7#_ e-tel-pi# [...]',\n",
       "  'translate Akkadian to English: u3 s,il2-lam ra-asz-ba-am _{gesz}kiri6_-sza i-ki-is',\n",
       "  'translate Akkadian to English: _ARAD2_-{d}suen _lugal_ larsa{ki}-ma _dumu_ ku-du-ur-ma-bu-uk',\n",
       "  'translate Akkadian to English: u3 a#-na#-ku8 ki-ma sa2!(KI)-bi-ia3-a#',\n",
       "  'translate Akkadian to English: _a_-szu2 sza2 {disz}szul#-lum-ma-a',\n",
       "  'translate Akkadian to English: szu-ma-am da-ri-a-am',\n",
       "  'translate Akkadian to English: u te-lit ka-ri-bi',\n",
       "  'translate Akkadian to English: a-na 5(u) 8(disz) ku3-babbar i#-szi#-[ma] x 7(disz) 1(u) 5(disz) [i-li]',\n",
       "  'translate Akkadian to English: szub-tu4 u3 szu-ku-su2 sza an-ni7 _lugal an_-e gir-gi-lu al-la-ku sza {d}en-lil2 _en kur-kur_ mu-um u3 su-hur-ma-szu a-szi-ir-tu4 _gal_ sza2 {d}e2-a {d}szul-pa-e3 {d}isz-ha-ra u3 {d}a-ru-ru us-qa-ru _bu-gi-na_ ma-gur-ru sza {d}suen ni-ip-hu nam-ri-ru sza _di-ku5 gal_ {d}utu is-qar-ru-ur-tu4 pur-ru-ur-tu4 sza {d}isz-tar _gaszan kur-kur_ bu-ru ek-du sza {d}iszkur _dumu_ an-ni7 {d}gira3 ez-zu szip-ru sza {d}nusku {d}szu-qa-mu-na u3 {d}szu-ma-li-ia _dingir-mesz_ mur2-ta-mu {d}nirah szip-ru sza {d}isztaran {d}szar3-ur4-<<ur4>> {d}szar3-gaz u3 {d}mes-lam-ta-e3',\n",
       "  'translate Akkadian to English: _ku3-babbar_ mi-szi-il',\n",
       "  'translate Akkadian to English: _zu2-kesz2 mu 1(disz)-kam_',\n",
       "  'translate Akkadian to English: _dumu_ dingir-szu-ib-ni-szu',\n",
       "  'translate Akkadian to English: [e] _du_',\n",
       "  'translate Akkadian to English: a-na {d}tiszpak',\n",
       "  'translate Akkadian to English: isz-t,up-dingir _szagina_ ma-ri2{ki}',\n",
       "  'translate Akkadian to English: {disz}ki-isz-{d}mar-du',\n",
       "  'translate Akkadian to English: u3 yu-um-ra-as,-dingir',\n",
       "  'translate Akkadian to English: [_BE samag_ i-na x x li]-sza#-ni-szu i-na _gub3 gar lu2_ szu-u2 mi-qi2#-it# [pi-i isz-sza-ra-ak-szu?]',\n",
       "  'translate Akkadian to English: ru-bu-u2 me-gir {d}5(u)',\n",
       "  'translate Akkadian to English: _a_ {disz}ARAD-{d}e2#-a',\n",
       "  'translate Akkadian to English: szum-ma _masz2_ a-na di-a-szi-im i-gur',\n",
       "  'translate Akkadian to English: [_e2_] ri-ba-tum _lukur# {d}utu#_',\n",
       "  'translate Akkadian to English: {d}[na]-ra-am-{d}suen',\n",
       "  'translate Akkadian to English: u3 {d}na-bi-um-be-el-numun _sza3 erin2 nig2-szu_ szu-i3-li2-szu it-ti-szu il-qi2 a-na pi2-tim sza bara2-mar il-li-ku-nim u3 _8(disz) erin2_ su-ti-i {disz}pir-hi-{d}mar-tu _lu2_ nibru{ki} u3 {d}na-bi-um-be-el-ze-ri _sza3 erin2 nig2-szu_ szu-i3-li2-szu pi2-tam sza bara2-mar u2-sze-bi-ru a-na _gal-ukken-na_ sza il-li-kam ip-qi2-is-su2-nu-ti-ma a-na bad3#-{[d]}suen-mu#-ba-li2-it,{ki} il-te-qu2-szu-nu-ti {disz}ib-[ni-{d}]suen# _gal-ukken-na_ qa2-du _erin2_ pi2#-ih#-ri#-[im i-tu-ur] _erin2_ a#-si-ri ma-ah-ri-ni ni-na-as,#-[s,a-ar] _u8-udu-hi-a_ ni-it-ta-na-ap-[la-as-ma]',\n",
       "  'translate Akkadian to English: ki-ma u2-ma-am s,e-ri s,e-ra li-ir-pu-ud re-bi-it _iri_-szu a-a-ik-bu-us {d}nin-urta be-el ap-li szu-u2-mi u3 ku-du-ur-ri ap-la-am na-aq me-e li-ki-im-szu-ma _sze numun_ u3 pe-er-a a-a-u2-szar-szi-szu {d}gu-la _gaszan_-tu szur-bu-tu4 e-tel-le-et ka-la be-le-e-ti s,a-ar-ri-sza si-im-ma la-az-za la te-e-ba-a ina zu-um-ri-szu lisz-ku-un-ma a-di u4-um bal-t,u szar-ka u da-ma ki-ma me-e li-ir-muk _dingir-mesz gal-mesz_ ma-la i-na _na4-ru2-a_ an-ni-i szu-um-szu-nu za-ak-ru szu-ba-tu-szu2-nu ud-da-a _{gesz}tukul-mesz_-szu-nu ku-ul-lu-mu u3 u2-s,u-ra-tu-su2-nu',\n",
       "  'translate Akkadian to English: s,a-bi-im a-na sza-ba-s,i2-im{ki} at,-t,a3-ra-ad',\n",
       "  'translate Akkadian to English: _a-sza3_ u3 _e2_ it-ta-lak',\n",
       "  'translate Akkadian to English: ib-ni-ma'],\n",
       " 'target': ['“Nabû is the protector of the kudurru',\n",
       "  'son of ...x-ushur; Belshunu,',\n",
       "  'Naram-Sîn. king of the world quarters the four, when HARshamat he defeated, and a wild bull within the Tibar mountain he alone did fell, an image of himself he created, and to Enlil his father, he dedicated it. The one who the inscription this one shall remove, Enlil and Shamash his foundation may they tear out, and his seed may they pluck up.',\n",
       "  'Uruk',\n",
       "  'If, in an enclosure,',\n",
       "  'pluck up.',\n",
       "  'They will finnish their days',\n",
       "  '(according to) 1 large cubit,',\n",
       "  'Tablets concerning the destiny (last will) of Agua',\n",
       "  'in one day',\n",
       "  '“What? Should we destroy that which we have built?',\n",
       "  'After him I went (and) his oxen, sheep, (and) property without measure brought away. His cities',\n",
       "  'during his lifetime',\n",
       "  'has seen shall seize her,',\n",
       "  \"Luh'ish'an\",\n",
       "  'If (there is) a mole on the middle of the nape (of his neck), the man . . .',\n",
       "  'opposite your breast, and by means of which you observe',\n",
       "  'If a man silver for/to his/its ...,',\n",
       "  'Regent of Elam and Shimashgi,',\n",
       "  'of Ki-x-enash,',\n",
       "  'foremost son',\n",
       "  'And if a slave of the palace or a slave of a commoner a daughter of an awilum married, and when he married her together with the dowry from the household of her father to the household of the slave of the palace or of the slave of the commoner entered, and subsequent to the time that they move in together a household they established, possessions accumulated after which either the slave of the palace or the slave of the commoner to his fate has gone, the daughter of an awilum her dowry shall take; furthermore, everything that her husband and she subsequent to the time that they moved in together accumulated into two parts they shall divide, and half the owner of the slave shall take, half the daughter of an awilum',\n",
       "  'If (there is) a mole on the nape of (his neck) . . .',\n",
       "  'The day of the goddess’s procession was profit and gain to me. The king’s prayer-that was my joy, And the accompanying music became a delight for me. I instructed my land to keep the god’s rites, And provoked my people to value the goddess’s name. I made praise for the king like a god’s, And taught the populace reverence for the palace. I wish I knew that these things were pleasing to one’s god! What is proper to oneself is an offence to one’s god, What in one’s own heart seems despicable is proper to one’s god, Who knows the will of the gods in heaven? Who understands the plans of the underworld gods?',\n",
       "  'and a lawsuit',\n",
       "  'grain to his creditor',\n",
       "  'and  his praised(?) water,',\n",
       "  'shall repay. 5 iku of quality field',\n",
       "  'his property',\n",
       "  'treasurer (in the temple) of',\n",
       "  'Nebuchadnezzar, king of Babylon.',\n",
       "  'up to Larsa,',\n",
       "  'brought to him',\n",
       "  '1 house of the pure man whom with him',\n",
       "  '(month) Nisanu, 6th day,',\n",
       "  'as wage of a woven-textile worker,',\n",
       "  'crossed over. The leader of those troops (is) Mutu-adgim. Hulalum to Allahada for kingship in place of Atamrim they will lead. Now the report that I heard to you I have sent. Take action! And this report, that I sent to you, quickly to the king let it arrive! to ... for ... Since the confirmation ...',\n",
       "  'If an alewife: criminals in her house have gathered, and those criminals she has not seized to the palace led off, that alewife shall be killed.',\n",
       "  'Iddin-narim,',\n",
       "  'or a slave',\n",
       "  'before them veils her,',\n",
       "  'and as he rises,',\n",
       "  'Nanna, king of heaven and earth are you!',\n",
       "  'With the curses,',\n",
       "  'at the gate of the storehouse he shall repay. Witnesses: Nergal-ina-teshi-etir, son of Balassu; Ea-shum-ibni, son of Kudurri; Silim-ili, son of Labashi; and the scribe Nabu-mukin-apli, son of Marduk-erēsh, descendant of Re’i-alpi; Larsa, of Nisanu the 4th day, 39th year of Nebuchadnezzar, the king.',\n",
       "  'he should stretch,',\n",
       "  'he made firm for me,',\n",
       "  'thus (say) your servants.',\n",
       "  'a man against his wife a penalty',\n",
       "  '(or?) another',\n",
       "  'Before Adad-manshum, military officer;',\n",
       "  'the buyer',\n",
       "  '[they have placed (there). In ... of the naru canal of Etel-pû ...',\n",
       "  'and cut down the awe-inspiring protection, her grove.',\n",
       "  'Warad-Sîn, king of Larsa, son of Kudur-mabuk.',\n",
       "  'but as for me, that my feet',\n",
       "  'son of Shullummaya,',\n",
       "  '(Thus) the eternal name',\n",
       "  'as well as (any) income from the devotees;',\n",
       "  'to 58 raise, the silver: n 7 15 will come up.',\n",
       "  'The seat and the tiara of Anu, king of the heavens; the girgilu-bird, the courier of Enlil, lord of the lands; the mum and the goatfish, the great socle of Ea; Shulpa’e, Ishhara, and Aruru; the crescent, the trough (and) the ship of Sîn; the disk, the radiance of the great judge, Shamash; the pointed star emblem of Ishtar, lady of the lands; the fierce calf of Adad, son of Anu; the furious fire god, the messenger of Nusku; Shuqamuna and Shumaliya, the gods who love each other; Nirah, the messenger of Ishtaran; Sharur, Shargaz, and Meslamta’e;',\n",
       "  'silver of half',\n",
       "  'the rent for 1 year,',\n",
       "  'son of Ilshu-ibnishu,',\n",
       "  'may he not walk.',\n",
       "  'For Tishpak,',\n",
       "  'Ishtup-Ilum, military governor of Mari.',\n",
       "  '“Qish-Amurrim,',\n",
       "  'and Yumrash-El,',\n",
       "  'If there is a mole on the left side of his tongue, that man will be subjected to blasphemy.',\n",
       "  'prince favored by Enlil,',\n",
       "  'son of ARAD-Ea,',\n",
       "  'If he a goat for threshing rented,',\n",
       "  'The house of Ribbatum, naditu of Shamash',\n",
       "  'Naram-Sîn.',\n",
       "  'and Nabium-bel-zeri from the troops under the supervision of Shu-ilishu, he took with him. They went to the opening of the shrine Parak-mar-(Enlil). and the 8 Sutaean soldiers, Pirhi-Amurrum the man of Nippur, and Nabium-bel-zeri from the troops under the supervision of Shu-ilishu they brought across the opening of the shrine Parak-mar-(Enlil). He entrusted them to the (other) commander who came (there). (and) they took them  to Dur-Sîn-muballit. Ibni-Sin the commander [turned back] with the troop of conscripts. We are taking [care of] the troop of prisoners. We are watching over the sheep and goats, but',\n",
       "  'like a wild beast the steppe may he roam, the square of his city may he not tread; Ninurta, lord of the heir, the name and the kudurru, the heir, the pourer of water, may he take away from him and seed and offspring may he not let him have; Gula, the greatest lady, supreme (among) all the ladies, a spreading, persistent, incurable wound, on his body may she place and until the day that he is alive, in pus and blood like water may he bathe; all the great gods whose name on this stela is invoked, whose (altar) bases are recognizable, whose weapons are visible, and whose figures',\n",
       "  'troops, to Shabashshim I have sent.',\n",
       "  'a field and house has gone,',\n",
       "  'he created, and']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_translations[1120:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 16370\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 1819\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = all_translations.train_test_split(test_size=0.1)\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 1819\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tests = translations[\"test\"]\n",
    "original_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['translate Akkadian to ']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3535e49515b4113ae4b15d331492e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 1819\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_starts = [f\"translate {lang_full[s]} to \" for s in source_langs]\n",
    "print(test_starts)\n",
    "\n",
    "def should_test(t):\n",
    "    return any(t[\"source\"].startswith(s) for s in test_starts)\n",
    "\n",
    "translations[\"test\"] = original_tests.filter(should_test)\n",
    "translations[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, model_max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad <pad> 0\n",
      "eos </s> 1\n",
      "unk <unk> 2\n"
     ]
    }
   ],
   "source": [
    "print(\"pad\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"eos\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(\"unk\", tokenizer.unk_token, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201b74f9553b4a299de1cd70ca5ddace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13959, 4823, 1258, 8603, 12, 1566, 10, 3, 834, 76, 591, 209, 599, 26, 159, 172, 61, 18, 157, 265, 834, 3, 23, 18, 52, 76, 4663, 18, 6306, 76, 115, 908, 1]\n",
      "[8, 166, 239, 6, 3, 88, 56, 2058, 117, 1]\n",
      "29 10 2.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65d1ee65193447d81e634f8f06f6f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 16370\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1819\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccc = 0\n",
    "sum_src_chars_per_token = 0.0\n",
    "num_src_chars_per_token = 0\n",
    "sum_tgt_chars_per_token = 0.0\n",
    "num_tgt_chars_per_token = 0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global ccc, sum_src_chars_per_token, sum_tgt_chars_per_token, num_src_chars_per_token, num_tgt_chars_per_token\n",
    "#     print(examples)\n",
    "    inputs = [example for example in examples[\"source\"]]\n",
    "    targets = [example for example in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=model_max_length, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=model_max_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    nexamples = len(inputs)\n",
    "    for i in range(nexamples):\n",
    "        nchar = len(inputs[i])\n",
    "        ntoks = len(model_inputs[\"input_ids\"][i])\n",
    "        if ntoks > 0:\n",
    "            sum_src_chars_per_token += nchar / ntoks\n",
    "            num_src_chars_per_token += 1\n",
    "        nchar = len(targets[i])\n",
    "        ntoks = len(model_inputs[\"labels\"][i])\n",
    "        if ntoks > 0:\n",
    "            sum_tgt_chars_per_token += nchar / ntoks\n",
    "            num_tgt_chars_per_token += 1\n",
    "    \n",
    "    ccc += 1\n",
    "    if ccc == 1:\n",
    "        print(model_inputs[\"input_ids\"][0])\n",
    "        print(model_inputs[\"labels\"][0])\n",
    "        nchar = len(targets[0])\n",
    "        ntoks = len(model_inputs[\"labels\"][0])\n",
    "        print(nchar, ntoks, nchar / ntoks)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_translations = translations.map(preprocess_function, batched=True)\n",
    "tokenized_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_src_chars_per_token = 1.9726247610618712\n",
      "avg_tgt_chars_per_token = 2.7860413785145792\n"
     ]
    }
   ],
   "source": [
    "avg_src_chars_per_token = sum_src_chars_per_token / num_src_chars_per_token\n",
    "avg_tgt_chars_per_token = sum_tgt_chars_per_token / num_tgt_chars_per_token\n",
    "print(\"avg_src_chars_per_token\", \"=\", avg_src_chars_per_token)\n",
    "print(\"avg_tgt_chars_per_token\", \"=\", avg_tgt_chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 16370\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1819\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_translations[\"train\"] = tokenized_translations[\"train\"].remove_columns([\"source\", \"target\"])\n",
    "tokenized_translations[\"test\"] = tokenized_translations[\"test\"].remove_columns([\"source\", \"target\"])\n",
    "tokenized_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 305)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_max_length = max([len(x[\"input_ids\"]) for x in tokenized_translations[\"train\"]])\n",
    "target_max_length = max([len(x[\"labels\"]) for x in tokenized_translations[\"train\"]])\n",
    "source_max_length, target_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 166, 239, 6, 3, 88, 56, 2058, 117, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_translations[\"train\"][0][\"labels\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(finetune_model_id if len(finetune_model_id)>0 else base_model_id, \n",
    "                                              max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"/home/fak/Projects/CuneiformTranslators/results/t5-base-p-akksux-en-20220722-173018/checkpoint-241500\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 3072,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"max_length\": 512,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.19.4\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "# data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../results/{model_id}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2*2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=has_cuda,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_translations[\"train\"],\n",
    "    eval_dataset=tokenized_translations[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fak/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16370\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 61410\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpraeclarum\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fak/Projects/CuneiformTranslators/tools/wandb/run-20220725_170444-17dfgyo3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/praeclarum/huggingface/runs/17dfgyo3\" target=\"_blank\">../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500</a></strong> to <a href=\"https://wandb.ai/praeclarum/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30828' max='61410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30828/61410 1:06:48 < 1:06:16, 7.69 it/s, Epoch 15.06/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.094100</td>\n",
       "      <td>0.792806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>0.788036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.957300</td>\n",
       "      <td>0.785010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.782949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.826800</td>\n",
       "      <td>0.773469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.773726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.771519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.770516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.765769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.768286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.766139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.560400</td>\n",
       "      <td>0.768100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.541500</td>\n",
       "      <td>0.765107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.763509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.771488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-4500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-6500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-8500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-10500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-12500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-14500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-16500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-18500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29000\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30000/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30500\n",
      "Configuration saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30500/config.json\n",
      "Model weights saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-base-p-f-akk-en-20220725-170404-t5-base-p-akksux-en-20220722-173018-checkpoint-241500/checkpoint-29000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1819\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TranslationPipeline(model=model.to(\"cpu\"), tokenizer=tokenizer, max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x7f40af0d6cb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'hu-mu-um dnanna-mu'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"translate English to French: hello my name is Frank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate Sumerian to English: {d}nin#-[...] x x AN# [...]\n",
      "--------------------------------------------------------------------------------\n",
      "Ninhursag(?) ...\n"
     ]
    }
   ],
   "source": [
    "source_test = translations[\"test\"][0][\"source\"]\n",
    "target_test = translations[\"test\"][0][\"target\"]\n",
    "print(source_test)\n",
    "print(\"-\"*80)\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Ningublaga, the lady of the mountain range, the mountain range of the mountain range, the mountain of the king, the mountain of the king, the mountain of the king, the mountain of the lands'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(text):\n",
    "    return pipeline(text)\n",
    "\n",
    "translate(source_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: The deliberations of the elders and juniors ... before ... the utterance(?) ...\n",
      "TARGET [...] lu2#? banda3{+da} sza3 kusz2-u3-bi [...] igi-sze3 ka ba x [...]\n",
      "PRED   ab-ba tur-tur-bi igi-sze3 [...] du11-ga x [...]\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: {d}nin#-[...] x x AN# [...]\n",
      "TARGET Ninhursag(?) ...\n",
      "PRED   Ningublaga, the lady of the mountain range, the mountain range of the mountain range, the mountain of the king, the mountain of the king, the mountain of the king, the mountain of the lands\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: [u4 ul4-li2-a-sze3] pa bi2-e3 %a an s,e-a-tim u2-szu-[pi2]\n",
      "TARGET I (thus) made resplendent forever.\n",
      "PRED   I made resplendent forever.\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: _e2-gal_ lu2-{d}marduk _lugal_\n",
      "TARGET the palace of Amēl-Marduk, the king.\n",
      "PRED   palace of Aml-Marduk, king\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: la-la-ra u3-na-a-du11 sze sa2-du11 sila-a 3(asz) gur-am3 ri-ba-ga-da he2-na-ab-szum2-mu\n",
      "TARGET To Lala speak! Barley, regular rations, in the road, 3 gur: Rigabada should give it to him!\n",
      "PRED   If you speak to Lala, the barley of the regular offering, in the street, is 3 gur, may it be given to Ribaga.\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: {gesz}gag x-[...]\n",
      "TARGET peg of the [... of the plow]\n",
      "PRED   The peg of ...\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: 17 laborers, 3 laborers, sick, foreman: Lugal-mumag;\n",
      "TARGET 1(u) 7(disz) gurusz 3(disz)# [gurusz] tu#-[ra] ugula lugal-mu-ma#-ag2#\n",
      "PRED   1(u) 7(disz) gurusz 3(disz) gurusz tu-ra ugula lugal-mu-ma-ag2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: 60 litres (of barley) 4 units (for) Lu-hubima\n",
      "TARGET 1(barig)# 4(disz)# lu2#-hu#-bi2#-ma#\n",
      "PRED   1(barig) 4(disz) lu2-hu-bi-ma\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Basket-of-tablets: debts of barley are here;\n",
      "TARGET pisan-dub-ba la2-ia3 sze i3-gal2\n",
      "PRED   pisan-dub-ba la2-ia3 sze i3-gal2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: The one who after(?) he fashioned a statue as in ancient times\n",
      "TARGET {uruda}alan u4 ul-a#-sze3# [...]\n",
      "PRED   alan u4 ul-li2-a-gin7 dim2-ma-a\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: nam-ti sza3 du10-ga\n",
      "TARGET and happy life\n",
      "PRED   and a life of happiness\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: My grave was waiting, and my funerary paraphernalia ready,\n",
      "TARGET pe-ti _ki-mah_ er-su-u2 szu-ka-nu-u-a\n",
      "PRED   ki-ma-ri i-ta-as,-ba-at szu-ut-ti-ia ip-te-e-em\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: A throne whose limbs were limbs of exceeding quality, was thus set up for Enlil, my king(?)\n",
      "TARGET {gesz}[gu-za ... {d}]en#?-lil2#? [...]\n",
      "PRED   geszgu-za me-dim2-bi me-dim2-ma diri-ga den-lil2 lugal-gu10-ra ur5-gin7 he2-ni-in-gub\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: e2-an-na-tum2-ma\n",
      "TARGET Eannatum,\n",
      "PRED   Eannatuma,\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: (and) ... (for) Ur-Hendursag, being her children;\n",
      "TARGET ($ blank space $) ur!-{d}hendur-sag dumu-ni-me\n",
      "PRED   [...] ur-dhendur-sag dumu-ni-me\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: to the year: “Urbilum,”\n",
      "TARGET mu ur-bi2-lum{ki}-sze3\n",
      "PRED   mu ur-bi2-lumki-sze3\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: It was written on his shoulder.\n",
      "TARGET za3-ga-[na a-ab-sar]\n",
      "PRED   i-na za-a-ti-szu sza-at,-ra-am\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Lugal-inimgina\n",
      "TARGET lugal-inim-gi-na dub-sar\n",
      "PRED   lugal-inim-gi-na\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: im 2(disz)-kam\n",
      "TARGET 2nd tablet;\n",
      "PRED   2nd tablet;\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: sag-ge6-ga-na a2 mi-ni-in-mah\n",
      "TARGET making him the mightiest among his black-headed people.\n",
      "PRED   he made it surpassing for his black-headed people.\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: [...] min-bi#?-[...]\n",
      "TARGET He sharpened along both edges of the agasilig ax\n",
      "PRED   The two of them ...\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: and mourning\n",
      "TARGET u3 di-im-ma-tim\n",
      "PRED   u3 ni-isz ta-ah-ta-szu\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: Iasmah-Addu -\n",
      "TARGET {disz}ia-as2-ma-ah-{d}iszkur\n",
      "PRED   ia-as2-ma-ah-diszkur\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: May his(?) “true wild cow” chase that man away, may you seize that man\n",
      "TARGET {d}sumun2-zi-de3-ni lu2 he2-eb2-sar-re lu2 he2-em-ib2-dab5-be2-[...]\n",
      "PRED   ab2 zi-da-ni lu2-bi he2-em-sar lu2-bi he2-dab5-be2-en\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: That which enters (into the temple)\n",
      "TARGET nig2 ku4#-ku4#\n",
      "PRED   nig2 ku4-ku4-da\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: 2(asz@c) geme2\n",
      "TARGET 2 female workers\n",
      "PRED   2 female laborers,\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Alala examined him;\n",
      "TARGET a2-la-la igi-ni i3-szi-gar\n",
      "PRED   a-la-la igi mu-na-ni-du8\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: 12 “HAR” objects of pear trees;\n",
      "TARGET 1(u) 2(disz) HAR {gesz}pesz3\n",
      "PRED   1(u) 2(disz) HAR geszu3-suh5\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: sza3 uri5{ki}-ma#\n",
      "TARGET in Ur,\n",
      "PRED   in Ur;\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: my protective spirit\n",
      "TARGET la-ma-si2\n",
      "PRED   na-asz2-pi2-ia\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: i-di3#{di}-nu\n",
      "TARGET did give,\n",
      "PRED   Iddinu,\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: <_3(disz) ezem {d}utu_> pi2-qi2-it-tam i-pa-qi2-id _igi_ e-tel-pi4-{d}na-bi-um _dumu_ {d}nanna-sag-kal_ _igi_ ip-qa2-tum _dumu_ pa-ha-lu-um _igi_ nu-ur2-i3-li2-szu# _dumu_ si2-ia-tum _igi_ na-ka-rum _dumu_ an-pi4-{d}a-a#\n",
      "TARGET at the three festivals for Shamash as provision he will provide. Before Etel-pî-nabium, son of Nanna-sigkal; before Ipqatum, son of Pahallum; before of Nur-ilishu, son of Siyatum; before Nakarum, son of Annum-pî-aya.\n",
      "PRED   (and) the 3rd festival of Shamash, the offering (offering) will be given. Before Etel-pî-nabium, son of Nanna-sagkal; before Ipqatum, son of Pahallum; before Nur-ilishu, son of Siyatum; before Nakarum, son of Annum-pî-aya.\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: the mass of the that land and the Turukkian men\n",
      "TARGET [um]-ma#-a-at ma#-a-tim sza-a-ti u3 _lu2-mesz_ tu-ru-uk-ki#-[i]\n",
      "PRED   _masz2 ma-da_ sza-a-ti u3 _lu2-mesz_ tu-ru-uk-ku-u2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: unparalleled battle\n",
      "TARGET szen-szen sag gi4-a\n",
      "PRED   me3 za3 nu-dab5\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: 8(disz)\n",
      "TARGET (total:) 8\n",
      "PRED   (total:) 8.\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: {d}nin-in-dub-ba\n",
      "TARGET For Nininduba,\n",
      "PRED   For Nininduba,\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: a bolt;\n",
      "TARGET {gesz}suhub3 ig\n",
      "PRED   geszsag-kul\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: um-ma-a _pad_-ma ul ba-ni _kasz-sag_-ma\n",
      "TARGET the following: “The food rations are not good. And the first-quality beer\n",
      "PRED   Thus (says): “The milk was not made, the beer was not made, and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: If a man ... grain ... gave, and then the grain into silver has converted, at harvest the grain and the interest on it, per 1 kor, 1 barig 4 ban2 he shall take\n",
      "TARGET szum-ma _lu2 sze_-a-am a-na x x x id-di-in-ma _sze_-a-am a-na _ku3-babbar_ i-te-pu!-usz i-na e-bu-ri _sze_-a-am u3 _masz-bi 1(asz) gur 1(barig) 4(ban2)_ i-le-eq-qe2\n",
      "PRED   szum-ma a-wi-lum _sze_ [...] id-di-in-ma _sze_ a-na _ku3-babbar_ u2-te-ep-pe2 i-na _buru14 sze_ u3 _masz2_-szu _1(asz) gur 1(barig) 4(ban2)_ i-le-qe2\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: abul 3(disz)-kam-ma ku4-ku4-da-ni-ta\n",
      "TARGET When she entered the 3rd gate,\n",
      "PRED   When she entered the 3rd gate,\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: _i7-i7_-szu\n",
      "TARGET his rivers\n",
      "PRED   his river\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: ki bad3?-ka {gesz}al-lu5-hab2-am3\n",
      "TARGET At the site of the fortification, it is an alluhab net(?)\n",
      "PRED   It is an alluhab tree in the site of the wall(?)\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: You will bring a suitably glad liver and rejoicing heart into his palace\n",
      "TARGET e2-gal-a-na urx(AH) sa6-ga sza3# hul2#-la# szu du7 mu-ne-DU#-en\n",
      "PRED   ur5 sza3 hul2-la si sa2-a e2-gal-la-na ku4-ku4-de3-en\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Bison of the right and left, a great work ...\n",
      "TARGET alim-ma zi-da gab2-bu-kam#? kin2 gal szu dab-dab-be2#\n",
      "PRED   szeg9 zi-da gab2-bu kin gal [...]\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: # addir ~ ($ blank space $) = %a isz-di-hu-um\n",
      "TARGET profit\n",
      "PRED   to add\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: and Ishtar, the lade of battle,\n",
      "TARGET u3 esz18-dar be-la2-at ta-ha-zi-im\n",
      "PRED   u3 dinanna na-asz2-pe2-e _me3_\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: a#-ru#-a# ur!-{d}ab-u2\n",
      "TARGET donated (by) Ur-Abu;\n",
      "PRED   donated (by) Ur-Abu;\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: total: 1 ash-c-(worker), corporate slave, not took in charge;\n",
      "TARGET szunigin 1(asz@c) gurusz nu-dab\n",
      "PRED   szunigin 1(asz@c) gurusz nu-dab5\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: and poured much wine into it.\n",
      "TARGET te mu-ni-de2-de2\n",
      "PRED   gesztin szar2-ra bi2-in-de2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Basket-of-tablets: xxx xxx xxx xxx xxx xxx\n",
      "TARGET pisan-dub-ba e2-tum sze siki tug2 i3 uruda u3 a2 nam-lu2-u18 ki lu2-gi-na szabra e2-amar-ra e2-{d}szara2{ki} iti masz-da3-gu7\n",
      "PRED   pisan-dub-ba nig2-ka9-ak a2 erin2-na szabra sanga-ne i3-gal2 mu en eriduki ba-hun\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: sza3 szeg9-bar-ra lu2 igi nu-bar-re-dam abgal-zu siki bar-ra bi2-in-du8 eridu{ki} {d}en-ki-ke4 ki ag2-ga2-ni e2-engur-ra sza3-bi he2-gal2 sug4-ga abzu zi kalam-ma ki ag2 {d}en-ki-ke4 e2 za3-ga du3-a me galam-ma tum2-ma eridu{ki} gissu-zu ab-sza3-ga la2-a a-ab-ba zi-ga gaba-szu-gar nu-tuku i7 mah ni2-gal2-la su kalam-ma zi-zi e2-engur-ra uru2 mah ki us2-sa e2 da engur-ra pirig abzu sza3-ga e2 mah {d}en-ki-ke4 gesztu2 kalam-e szum2-mu akkil-zu i7 mah zi-ga-gin7 lugal {d}en-ki-ra mu-un-na-tuku-am3 e2 ku3-ga-ni-sze3 du10-bi mu-un-ga2-ga2\n",
      "TARGET lets nobody look into its midst. Your abgal priests let their hair down their backs. Enki's beloved Eridu, E-engura whose inside is full of abundance! Abzu, life of the Land, beloved of Enki! Temple built on the edge, befitting the artful divine powers! Eridu, your shadow extends over the midst of the sea! Eridu, your shadow extends over the midst of the sea! Rising sea without a rival; mighty awe-inspiring river which terrifies the Land! E-engura, high citadel (?) standing firm on the earth! Temple at the edge of the engur, a lion in the midst of the abzu; lofty temple of Enki, which bestows wisdom on the Land; your cry, like that of a mighty rising river, reaches (?) King Enki. To his silver house its goodness he sets,\n",
      "PRED   The heart of a wild bull is not to be seen, so your abgal priest loosens his hair on the back. Enki loves Eridu, the E'engura temple, whose interior is full of abundance. Abzu, true homeland, beloved of Enki, built on the edge, fitting for the elaborate divine powers. Eridu, your shade extends into the midst of the sea. Rising up in the sea, unrivaled in the great river, whose radiance covers the country, E'engura, the great city, the foundation of the earth, the house beyond the engur canal, lion of the Abzu, the great house of Enki, given wisdom of the nation, your akkil priest like a rising river, for king Enki he has had, in his holy house he sets good food there.\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: me ul-li2-a ki-be2 gi4-gi4\n",
      "TARGET in order to restore the ancient divine attributes,\n",
      "PRED   restored the ancient divine powers,\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: 1(asz) 4(barig) 5(ban2) gur\n",
      "TARGET 1 gur 4 barig 5 ban2,\n",
      "PRED   1 gur 4 barig 5 ban2\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: {d}nanna lugal an-ki-ke4 dimma# kalam-ma tesz2 hu-mu-ra-ab-si-ge\n",
      "TARGET Nanna, king of the universe, shall put the mood(?) of the land in unison for you\n",
      "PRED   Nanna, king of heaven and earth, shall unify the mind of the land\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: mu-un-gur11-gu10 buru4{muszen}-dugud zi-ga-gin7 dal-dal-bi ha-ba-ab-in-zi mu-un-gur11-gu10 ga-am3-du11\n",
      "TARGET “My possessions, like a flock of rooks rising up, have risen in flight -- I shall cry “O my possessions.”\n",
      "PRED   My possessions have been piled up, like a francolin flying furiously, I must cry “My possessions!”\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: _kur_ ma-al-ma-li-isz i-zu-zu a-na tu-ur gi-mil-li\n",
      "TARGET (and) the land equally they divided. To avenge\n",
      "PRED   the entire land he inherited. To return to the gimilu irrigation district\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: [...]-gi4#?-e\n",
      "TARGET Was not returning (i.e., receding?) at either noon or evening\n",
      "PRED   The king, the king who was spared from the day(s)(?)\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: For Enki, the lord of Eridu,\n",
      "TARGET {d}en-ki lugal eridu{ki}-ra\n",
      "PRED   den-ki lugal eriduki-ra\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Shulgi-urugu,\n",
      "TARGET {d#}szul#-gi#-uru#-mu#\n",
      "PRED   dszul-gi-u4-ru-gu2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: he goes to the temple and speaks to it.\n",
      "TARGET e2-e im-ma-gen gu3 im-ma-de2-e\n",
      "PRED   e2-sze3 im-gen gu3 mu-na-de2-e\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: in-szi-in-sze-ga\n",
      "TARGET became agreeable,\n",
      "PRED   has agreed,\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: 74 oxen, grain-fed, 2nd quality,\n",
      "TARGET 1(gesz2) 1(u) 4(disz) gu4 niga us2\n",
      "PRED   1(gesz2) 1(u) 4(disz) gu4 niga us2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: to give cattle and sheep to that house of cattle,\n",
      "TARGET e2 gu4-ra2-be2 gu4 udu szum2-mu-de3\n",
      "PRED   e2 gu4-bi-sze3 gu4 udu szum2-mu-de3\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: a harp and a bronze kettledrum\n",
      "TARGET balag li#-li-is zabar#\n",
      "PRED   balag geszsag-kul zabar\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: egir-bi ab su3-a si# li-bi2-ib-sa2 kun# nu-mu-x-x\n",
      "TARGET Its end would not function properly in the deep(?) sea, (its) “tail would not undulate” (i.e., leave a wake)\n",
      "PRED   The behind did not align properly with the deep water, it did not ... the tail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: ki-la2-bi 1(disz) ma-na 1(u) 8(disz) gin2\n",
      "TARGET Their weight: 1 ma-na, 18 shekels.\n",
      "PRED   Its weight: 1 ma-na, 18 shekels.\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: lu2-mu igi im-mi-du8-am3\n",
      "TARGET my own man saw it!\n",
      "PRED   “Did you see my man?”\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Shu-Erra, scribe, son of Ishar-beli, is your servant.\n",
      "TARGET szu-er3-ra dub-sar dumu i-szar-be-li2 ARAD2-zu\n",
      "PRED   szu-er3-ra dub-sar dumu i-szar-be-li2 ARAD2-zu\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Basket-of-tablets: xxx xxx xxx\n",
      "TARGET pisan-dub-ba nig2-ka9-ak sza3-bi su-ga sza3 ze2 i3-gal2\n",
      "PRED   pisan-dub-ba nig2-ka9-ak sze erin2 szu-ku6 sza3 gu2-ab-baki\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: the first-born son\n",
      "TARGET dumu-sag\n",
      "PRED   dumu-sag\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: 1(barig)# 4(disz)# en-lil2-e-si\n",
      "TARGET 60 litres (of barley) 4 units (for) Enlil-esi\n",
      "PRED   60 litres (of barley) 4 units (for) Enlil-esi\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: With the midst/heart of a storm(?) ...\n",
      "TARGET sza3 {tu15}[...] a#? [...]\n",
      "PRED   sza3 u4-da x [...]\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: I am a wise one who has departed from everything (else, i.e., peerless?)\n",
      "TARGET nig2-nam-ta e3#-[a]-me#-en#\n",
      "PRED   ku3-zu nig2-nam-ta ba-ra-e3-me-en\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: 4, foreman of throne bearers;\n",
      "TARGET 4(asz) <ugula> gu-za-la2\n",
      "PRED   4(asz) ugula> gu-za-la2\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: mu# hu-uh2-nu-ri{ki} ba-hul\n",
      "TARGET year: “Huhnuri was destroyed.”\n",
      "PRED   year: “Huhnuri was destroyed.”\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: its barley: 83 gur 4 barig;\n",
      "TARGET sze-bi 1(gesz2)# 2(u) 3(asz) 4(barig) gur\n",
      "PRED   sze-bi 1(gesz2) 2(u) 3(asz) 4(barig) gur\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: ...,\n",
      "TARGET x x x-am#?{ki}\n",
      "PRED   [...] x x x x\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: Namzitara,\n",
      "TARGET nam-zi-tar-ra\n",
      "PRED   nam-zi-tar-ra\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: favored one of Shamash, beloved of Marduk, I— that which from days of old no king among kings had ever built, for Shamash my lord in a grand manner I did truly build for him.\n",
      "TARGET mi-gi4-ir {d}utu na-ra-am {d}marduk a-na-ku sza isz-tu _u4_-um s,i-a-tim _lugal_ in _lugal_-ri2 la ib-ni-u3 a-na {d}utu be-li2-ia ra-bi-isz lu e-pu-us2-su-um\n",
      "PRED   mi-gir dutu na-ra-am dmarduk a-na-ku sza isz-tu _u4-mesz_ ul _lugal_ in _lugal-mesz_ la i-pu-szu a-na dutu be-li2-ia ra-bi-isz lu e-pu-usz-szu\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: install! And in Zibnatum\n",
      "TARGET szu-ku-un u3 i-na zi-ib-na-tim{ki}\n",
      "PRED   szu-ku-un u3 i-na zi-ib-na-timki\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: pisan-dub-ba sze-ba siki-ba tug2-ba giri3-se3-ga sza3-iri u3 geme2 usz-bar {d}lamma {d}szu-{d}suen sza3 gu2-ab-ba{ki}\n",
      "TARGET Basket-of-tablets: xxx xxx xxx xxx\n",
      "PRED   Basket-of-tablets: xxx xxx xxx xxx\n",
      "------------------------------------------------\n",
      "QUERY  translate Akkadian to English: id-di-isz-szi\n",
      "TARGET gives her\n",
      "PRED   he gave her,\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: 1(disz) sila3 zu2-lum\n",
      "TARGET 1 sila3 dates,\n",
      "PRED   1 sila3 dates,\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: sza3-ge tum2-ma\n",
      "TARGET the one fit for the heart\n",
      "PRED   who is fit for the heart,\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: the release(?), like(?) wind(?).\n",
      "TARGET du8#? tu15-gin7#?\n",
      "PRED   tu15-de3 tu15-gin7\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: sze-bi 1(u) 5(asz) 4(barig) gur\n",
      "TARGET its barley: 15 gur 4 barig;\n",
      "PRED   its barley: 15 gur 4 barig;\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: inim du14-da-ka\n",
      "TARGET In words of competition,\n",
      "PRED   In the speech of the assembly\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: 2 billy goats,\n",
      "TARGET 2(disz) masz2\n",
      "PRED   2(disz) masz2\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: servant of Anam.\n",
      "TARGET _ARAD2_ an-am3\n",
      "PRED   _ARAD2_ an-am3\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: 5(u) 2(disz) masz-da3\n",
      "TARGET 52 gazelles,\n",
      "PRED   52 gazelles,\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: ... surface, outside, 1 eshe3 1 1/2 iku, inside;\n",
      "TARGET [n] GAN2 bar 1(esze3) 1(iku) 1/2(iku) ki\n",
      "PRED   [...] GAN2 bar 1(esze3) 1(iku) 1/2(iku) sza3-bi\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: kiszib3 ma-nu-um-ki-ma-e2-a\n",
      "TARGET Seal of Manum-kima-ea\n",
      "PRED   under seal of Manum-kima-ea;\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: ... to the border/extreme ... advance/withstand\n",
      "TARGET [...] za3-sze3 [... ru]-gu2\n",
      "PRED   [...] za3-sze3 x [...] x szu gid2-da\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: of the land\n",
      "TARGET ma-ti\n",
      "PRED   ma-tim\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Sumerian: elder brother(?) of ...,\n",
      "TARGET pa4 sag [x x x] x %a a-bi x x x x e x\n",
      "PRED   szesz#?-gal#? x-x-x\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: three: Shamash-nashir, son of Ili-itê,\n",
      "TARGET _3(disz)_ {d}utu-na-s,i-ir _dumu_ i3-li2-i-te-e\n",
      "PRED   3(disz) dutu-na-s,i-ir _dumu_ i3-li2-i-te-e\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: a2 dumu-gi7 szesz-tab-ba bala-a gub-ba#\n",
      "TARGET the production of “dumugi apprentices” in bala service.\n",
      "PRED   labor of the freed sons and the brother(s) stationed in the bala;\n",
      "------------------------------------------------\n",
      "QUERY  translate English to Akkadian: once his painful behaviour he has seen, should be absolved his crime;\n",
      "TARGET e-nu-ma ep-sze-ta-szu2 ma-ru-usz-tu2 i-mu-ru lip-pa-t,ir a-ra-an-szu2\n",
      "PRED   i-na na-asz-pa-ar-ti-szu sza i-ta-mar i-te-er-ta-szu li-ir-ta-szu\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: {d#}en#-lil2#-le#? sza3#? ki#? [ag2]-ga2#?-ni#-x e2#-x-[...]-x-[...]-x [...] x [...] x [...]\n",
      "TARGET Thus by means of the loving heart of Enlil the Enamtila temple was opened for me\n",
      "PRED   Enlil ... his beloved heart(?) ... the temple ...\n",
      "------------------------------------------------\n",
      "QUERY  translate Sumerian to English: sag-il2 na-du12-du12\n",
      "TARGET may he never hold (his) head high!\n",
      "PRED   and he shall raise his head.\n"
     ]
    }
   ],
   "source": [
    "tests = original_tests\n",
    "def sample(num_samples=100):\n",
    "    for i in range(min(num_samples, tests.num_rows)):\n",
    "        t = tests[i]\n",
    "    #     print(t)\n",
    "        src = t[\"source\"]\n",
    "        tgt = t[\"target\"]\n",
    "        query = src\n",
    "        pred = pipeline(query)[0][\"translation_text\"]\n",
    "        print(\"-\"*48)\n",
    "        print(\"QUERY \", query)\n",
    "        print(\"TARGET\", tgt)\n",
    "        print(\"PRED  \", pred)\n",
    "    #     break\n",
    "    \n",
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained\n",
      "Configuration saved in /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/config.json\n",
      "Model weights saved in /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/pytorch_model.bin\n",
      "tokenizer config file saved in /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/tokenizer_config.json\n",
      "Special tokens file saved in /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.abspath(f\"/home/fak/nn/Data/generated/cuneiform/{model_id}-fullytrained\")\n",
    "trainer.save_model(model_path)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/tokenizer_config.json\n",
      "Special tokens file saved in /home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/tokenizer_config.json',\n",
       " '/home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/special_tokens_map.json',\n",
       " '/home/fak/nn/Data/generated/cuneiform/t5-base-bi-p-akksux-en-20220723-023520-fullytrained/tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
