{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transliteration\n",
    "\n",
    "Based on: https://huggingface.co/docs/transformers/tasks/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import TranslationPipeline\n",
    "from datasets import load_dataset, Dataset, dataset_dict\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"TrainTransliteration.ipynb\"\n",
    "\n",
    "source_langs = set([\"akk\", \"sux\"])\n",
    "\n",
    "target_langs = set([\"akkts\", \"suxts\"])\n",
    "\n",
    "base_model_id = \"t5-small\"\n",
    "\n",
    "model_max_length = 128\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-small-sux-akk'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = f\"{base_model_id}-{'-'.join(source_langs)}\"\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f423676e8f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.cuda.device(0) if has_cuda else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-dc8b07d8fd701d7f\n",
      "Reusing dataset json (/home/fak/.cache/huggingface/datasets/json/default-dc8b07d8fd701d7f/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26312e1dec14903aa61b61dfbfed692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['p', 'a', 'l', 'sux', 'en', 'akk', 'akkts', 'de', 'suxts', 'fr', 'elx', 'es', 'it'],\n",
       "        num_rows: 74584\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_translations = load_dataset(\"json\", data_files=\"../data/translations.jsonl\")\n",
    "all_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0f2e1cab194a748c45bc9fcbfeffad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 7432\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourceandtargets = []\n",
    "for line in tqdm(all_translations[\"train\"]):\n",
    "    for s in source_langs:\n",
    "        for t in target_langs:\n",
    "            ls = line[s]\n",
    "            lt = line[t]\n",
    "            if ls is not None and lt is not None:\n",
    "                if lt[-1] == \".\" or lt[-1] == \"!\" or lt[-1] == \";\" or lt[-1] == \",\":\n",
    "                    lt = lt[:-1]\n",
    "                sourceandtargets.append((ls, lt))\n",
    "                \n",
    "random.shuffle(sourceandtargets)\n",
    "translations = Dataset.from_dict({\"source\": [x[0] for x in sourceandtargets], \"target\": [x[1] for x in sourceandtargets]})\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 6688\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 744\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = translations.train_test_split(test_size=0.1)\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 744\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = translations[\"test\"]\n",
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': ['1/2(disz) _ma-na ku3-babbar_',\n",
       "  'uz-ni-sza u2-na-ak-ka-as2',\n",
       "  'is-su#-uh ba-asz-ta-am',\n",
       "  '_lugal szar2-ra_',\n",
       "  'asz-szum i-na _e2_ szu-s,i2-im',\n",
       "  'ir-tum im-ha-s,u tu-le-e it,-t,e4-ru',\n",
       "  'u3 ha-la-aq ma-ti-szu li-isz-ku-un-szum',\n",
       "  'sza lib3-bi-sza u2-sza-as,-li-szi',\n",
       "  'sza it-ti be-li2-ia',\n",
       "  'u3 szum-ma _munus_ szi-it mi-ta-at'],\n",
       " 'target': ['mišil mina kaspam',\n",
       "  'uznīša unakkas',\n",
       "  'issuḫ baštam',\n",
       "  'šar kiššati',\n",
       "  'aššum ina bītim šūṣîm',\n",
       "  'irtī imḫaṣū tūlê iṭṭirû',\n",
       "  'u ḫalāq mātišu liškunšum',\n",
       "  'ša libbiša ušaṣliši',\n",
       "  'sza itti bēliya',\n",
       "  'u šumma sinniltu šīt mētat']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/blog/how-to-train\n",
    "* https://huggingface.co/docs/transformers/v4.20.1/en/fast_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3522c1e13d5143598c32e4fd1834a5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8c66b2dedd4a75a5bb95953b494c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_txt_lines = []\n",
    "\n",
    "for d in [\"train\", \"test\"]:\n",
    "    for t in tqdm(translations[d]):\n",
    "        tokenizer_txt_lines.append(t[\"source\"])\n",
    "        tokenizer_txt_lines.append(t[\"target\"])\n",
    "        \n",
    "len(tokenizer_txt_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fak/Projects/CuneiformTranslators/tools/tokenizer_training_data.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_txt_path = os.path.abspath(\"tokenizer_training_data.txt\")\n",
    "with open(tokenizer_txt_path, \"wb\") as f:\n",
    "    f.write(bytes(\"\\n\".join(tokenizer_txt_lines), \"utf8\"))\n",
    "tokenizer_txt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li-ip-li-pi2\r\n",
      "liplippim\r\n",
      "u2-gal-lib2-ma kal pe-er-ti szu-me-lam [...]\r\n",
      "ugallibma kal perti šumelam ..\r\n",
      "i-na-ad-di-in\r\n",
      "inaddin\r\n",
      "sza qi2-bit pi-i-szu\r\n",
      "ša qibīt pīšu\r\n",
      "[...]-nim [...]\r\n",
      ".."
     ]
    }
   ],
   "source": [
    "!tail tokenizer_training_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=[\"<pad>\", \"</s>\", \"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0>', '<extra_id_1>', '<extra_id_2>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_special_tokens = [f\"<extra_id_{i}>\" for i in range(100)]\n",
    "additional_special_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "12136 vocab_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/fak/Projects/CuneiformTranslators/results/transliteration_tokenizer.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "trainer = BpeTrainer(special_tokens=special_tokens + additional_special_tokens)\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"$0 </s>\",\n",
    "    pair=\"$A </s> $B:1 </s>:1\",\n",
    "    special_tokens=[(\"</s>\", 1)],\n",
    ")\n",
    "tokenizer.model_max_length=model_max_length\n",
    "files = [tokenizer_txt_path]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "print(tokenizer.get_vocab_size(), \"vocab_size\")\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "tokenizer_json_path = os.path.abspath(\"../results/transliteration_tokenizer.json\")\n",
    "tokenizer.save(tokenizer_json_path)\n",
    "tokenizer_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qāti bīt-muktaris-saḫ\n",
      "[804, 401, 112, 5439, 112, 2017, 1]\n"
     ]
    }
   ],
   "source": [
    "test_txt = tokenizer_txt_lines[3]\n",
    "test_tokens = tokenizer.encode(test_txt).ids\n",
    "print(test_txt)\n",
    "print(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qāti bīt - muktaris - saḫ'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qāti bīt - muktaris - saḫ </s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, model_max_len=model_max_length)\n",
    "tokenizer.decode(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# atokenizer = AutoTokenizer.from_pretrained(base_model_id, model_max_length=model_max_length)\n",
    "# atokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# atokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "    'eos_token': '</s>',\n",
    "    'unk_token': '<unk>',\n",
    "    'pad_token': '<pad>',\n",
    "    \"additional_special_tokens\": additional_special_tokens })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad <pad> 0\n",
      "eos </s> 1\n",
      "unk <unk> 2\n"
     ]
    }
   ],
   "source": [
    "print(\"pad\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"eos\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(\"unk\", tokenizer.unk_token, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12136"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7f4236777370> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5b458e40854880b456bba23f40dd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[767, 112, 272, 112, 208, 1]\n",
      "[4830, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69add377390f48ffb338913e41481d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6688\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 744\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccc = 0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global ccc\n",
    "#     print(examples)\n",
    "    inputs = [example for example in examples[\"source\"]]\n",
    "    targets = [example for example in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=model_max_length, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=model_max_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    ccc += 1\n",
    "    if ccc == 1:\n",
    "        print(model_inputs[\"input_ids\"][0])\n",
    "        print(model_inputs[\"labels\"][0])\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_translations = translations.map(preprocess_function, batched=True)\n",
    "tokenized_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 23)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_max_length = max([len(x[\"input_ids\"]) for x in tokenized_translations[\"train\"]])\n",
    "target_max_length = max([len(x[\"labels\"]) for x in tokenized_translations[\"train\"]])\n",
    "source_max_length, target_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4830, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_translations[\"train\"][0][\"labels\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5WithLMHeadModel\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.19.4\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = copy.deepcopy(base_model.config)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_config(model_config)\n",
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PreTrainedTokenizerFast(name_or_path='', vocab_size=12136, model_max_len=128, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}), model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(12136, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(12136, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(12136, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=12136, bias=False)\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../results/{model_id}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.75*2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=100,\n",
    "    fp16=has_cuda,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_translations[\"train\"],\n",
    "    eval_dataset=tokenized_translations[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/fak/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6688\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2700\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpraeclarum\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fak/Projects/CuneiformTranslators/tools/wandb/run-20220718_234142-36d6niy7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/praeclarum/huggingface/runs/36d6niy7\" target=\"_blank\">../results/t5-small-sux-akk</a></strong> to <a href=\"https://wandb.ai/praeclarum/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2700/2700 04:59, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.483838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.875815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.612282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.315233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.133185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.065056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.021270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.917338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.887601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.807712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.779590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.753612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.702974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.686335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.691062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.672513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.657252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.588760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.610204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.569595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.567361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.589459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.530808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.533931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.555813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.581981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.518507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.499393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.513083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.516788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.531232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.489976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.489787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.517941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.519111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.257300</td>\n",
       "      <td>4.504049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.474182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.489526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.501901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.499174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.476996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.510922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.485975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.528567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.473987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.475101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.466060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.487514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.475113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.497633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.488055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.485703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.498911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.060100</td>\n",
       "      <td>4.484274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.495169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.488517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.534429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.486129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.527277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.507838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.496760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.521842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.484469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.508136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.508846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.528336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.507248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.539765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.544432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.566094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.546105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.560247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>4.548214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.520780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.526751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.542822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.531522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.523855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.523732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.539554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.542629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.549269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.554814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.566930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.573099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.569989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.556498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.564841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.562320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.570096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>4.570794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.576260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.563486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.563239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.567388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.574997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.570581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.565990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>4.567884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ../results/t5-small-sux-akk/checkpoint-500\n",
      "Configuration saved in ../results/t5-small-sux-akk/checkpoint-500/config.json\n",
      "Model weights saved in ../results/t5-small-sux-akk/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-small-sux-akk/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-small-sux-akk/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-small-sux-akk/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ../results/t5-small-sux-akk/checkpoint-1000\n",
      "Configuration saved in ../results/t5-small-sux-akk/checkpoint-1000/config.json\n",
      "Model weights saved in ../results/t5-small-sux-akk/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-small-sux-akk/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-small-sux-akk/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-small-sux-akk/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ../results/t5-small-sux-akk/checkpoint-1500\n",
      "Configuration saved in ../results/t5-small-sux-akk/checkpoint-1500/config.json\n",
      "Model weights saved in ../results/t5-small-sux-akk/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-small-sux-akk/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-small-sux-akk/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-small-sux-akk/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ../results/t5-small-sux-akk/checkpoint-2000\n",
      "Configuration saved in ../results/t5-small-sux-akk/checkpoint-2000/config.json\n",
      "Model weights saved in ../results/t5-small-sux-akk/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-small-sux-akk/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-small-sux-akk/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-small-sux-akk/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ../results/t5-small-sux-akk/checkpoint-2500\n",
      "Configuration saved in ../results/t5-small-sux-akk/checkpoint-2500/config.json\n",
      "Model weights saved in ../results/t5-small-sux-akk/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ../results/t5-small-sux-akk/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ../results/t5-small-sux-akk/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [../results/t5-small-sux-akk/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, token_type_ids, source. If target, token_type_ids, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 744\n",
      "  Batch size = 256\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2700, training_loss=1.5257306501600478, metrics={'train_runtime': 301.673, 'train_samples_per_second': 2216.97, 'train_steps_per_second': 8.95, 'total_flos': 7097747012321280.0, 'train_loss': 1.5257306501600478, 'epoch': 100.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TranslationPipeline(model=model.to(\"cpu\"), tokenizer=tokenizer, max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x7f41b1b8c7c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '1 kurrum 2 qa šamnum ša aššata aššata naṭū naṭū naṭū naṭū'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"translate English to French: hello my name is Frank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2(disz) _ma-na ku3-babbar_\n",
      "--------------------------------------------------------------------------------\n",
      "mišil mina kaspam\n"
     ]
    }
   ],
   "source": [
    "source_test = translations[\"test\"][0][\"source\"]\n",
    "target_test = translations[\"test\"][0][\"target\"]\n",
    "print(source_test)\n",
    "print(\"-\"*80)\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mišil mina kaspam\n"
     ]
    }
   ],
   "source": [
    "def translate(text):\n",
    "    return pipeline(text)[0][\"translation_text\"]\n",
    "\n",
    "print(translate(source_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "QUERY  isz-tu _a-sza3_ u2-sze-bi-la-asz2-sze\n",
      "TARGET ištu eqle ušēbilašše\n",
      "PRED   eqel lā ištu\n",
      "------------------------------------------------\n",
      "QUERY  szad-di-ha a-ha-a-a ku-ta-at-tu-ma i-tah-za\n",
      "TARGET ﻿šaddiḫa aḫâya kutettuma itaḫzā\n",
      "PRED   adi šūnu ina gimiršunu kimin\n",
      "------------------------------------------------\n",
      "QUERY  _{gesz}ma2_ ut,-t,e4-bi\n",
      "TARGET elippam uṭṭebbi\n",
      "PRED   elippam uṭṭebbi\n",
      "------------------------------------------------\n",
      "QUERY  i-na su2-un\n",
      "TARGET ina sūn\n",
      "PRED   ina sūn ummišu\n",
      "------------------------------------------------\n",
      "QUERY  szi-pa-a-at#\n",
      "TARGET šipat\n",
      "PRED   umma šibtu\n",
      "------------------------------------------------\n",
      "QUERY  a-wi-lam szu-a-ti\n",
      "TARGET awīlam šuāti\n",
      "PRED   awīlam šuāti\n",
      "------------------------------------------------\n",
      "QUERY  il-qe2-ma\n",
      "TARGET ilqēma\n",
      "PRED   ilqēma\n",
      "------------------------------------------------\n",
      "QUERY  _1(disz) ma-na ku3-babbar_\n",
      "TARGET ištēn mina kaspam\n",
      "PRED   ištēn mina kaspam\n",
      "------------------------------------------------\n",
      "QUERY  la hal-qu-u2-ni _szesz-mesz_ mu-ti-sza\n",
      "TARGET lā halqūni aḫḫū mutiša\n",
      "PRED   lā mutiša ’’ bētiša\n",
      "------------------------------------------------\n",
      "QUERY  _gissu bad3_ lu-u2 man-za-zu-ka\n",
      "TARGET ṣillī dūri lū manzazūka\n",
      "PRED   lū šareštu\n",
      "------------------------------------------------\n",
      "QUERY  nu-du-un-na-am\n",
      "TARGET nudunnâm\n",
      "PRED   nudunnâm\n",
      "------------------------------------------------\n",
      "QUERY  {d}iszkur {d}szakkan {d}nisaba {d}utu {d}i7 hur#-[sa-an]\n",
      "TARGET adda šakkan nisaba šamaš id ḫursānu\n",
      "PRED   adad šamaš dayyān - addu\n",
      "------------------------------------------------\n",
      "QUERY  i-qer-ru-ma\n",
      "TARGET iqerrûma\n",
      "PRED   izēruma\n",
      "------------------------------------------------\n",
      "QUERY  lu-u2 i-na pu-uz-ri lu-u2 i-na s,a-al-te\n",
      "TARGET lū ina puzri lu ina ṣalte\n",
      "PRED   lū ina ṣalte ana pani ṣābē\n",
      "------------------------------------------------\n",
      "QUERY  gi-ir-gi-szu s,e2-ni-tum\n",
      "TARGET girgiššu ṣennītum\n",
      "PRED   girgišša ṣennītam\n",
      "------------------------------------------------\n",
      "QUERY  a-na be-el _e2_\n",
      "TARGET ana bēl bītim\n",
      "PRED   ana bēl bītim\n",
      "------------------------------------------------\n",
      "QUERY  i-be2-el-lu-u2\n",
      "TARGET ibellû\n",
      "PRED   lū amīlūtu\n",
      "------------------------------------------------\n",
      "QUERY  i-te-el-li\n",
      "TARGET ītelli\n",
      "PRED   ītelli\n",
      "------------------------------------------------\n",
      "QUERY  _iti_-szu la im-la-ma\n",
      "TARGET waraḫšu lā imlāma\n",
      "PRED   lā šuṣîmma\n",
      "------------------------------------------------\n",
      "QUERY  _gu4_ u3 lu _udu_\n",
      "TARGET alpam u lū immeram\n",
      "PRED   lu alpam u lū immeram\n",
      "------------------------------------------------\n",
      "QUERY  _4(barig) esir2?_ a-na _1(disz) gin2 ku3-babbar_\n",
      "TARGET 4(sūt) ittûm ana 1 šiqil kaspim\n",
      "PRED   1 kurrum ûm ana 1 šiqil kaspim\n",
      "------------------------------------------------\n",
      "QUERY  li#-bu-am-ma a-ia-szi-im\n",
      "TARGET libû’amma ayâšim\n",
      "PRED   išemmûma šī ’ ī\n",
      "------------------------------------------------\n",
      "QUERY  szi-im _gu4_ ba-al-t,im u3 _uzu gu4_ mi-tim\n",
      "TARGET šīm alpim balṭim u šir alpim mitim\n",
      "PRED   alpam u imēram\n",
      "------------------------------------------------\n",
      "QUERY  a-bu-sza hab-bu-ul ki-i sza-par2-te\n",
      "TARGET abuša ḫabbul kî šaparte\n",
      "PRED   ša kî ḫubulle\n",
      "------------------------------------------------\n",
      "QUERY  a-ra-an di-nim szu-a-ti\n",
      "TARGET aran dīnim šuāti\n",
      "PRED   dīnim šuāti\n",
      "------------------------------------------------\n",
      "QUERY  _su#_ mi-na-ti-ia [...]\n",
      "TARGET zumru minātiya ..\n",
      "PRED   ... šattim ištiat kakkī\n",
      "------------------------------------------------\n",
      "QUERY  asz-ta-ap-ra-am\n",
      "TARGET aštapram\n",
      "PRED   tašpuram\n",
      "------------------------------------------------\n",
      "QUERY  _bala_ ta-ne-hi-im\n",
      "TARGET palê tānēḫim\n",
      "PRED   tānēḫim\n",
      "------------------------------------------------\n",
      "QUERY  szum-ma isz-tu _u8 udu hi-a_\n",
      "TARGET šumma ištu ṣēnum\n",
      "PRED   šumma ištu māt ṣēnam\n",
      "------------------------------------------------\n",
      "QUERY  [...] a a [...]\n",
      "TARGET ..\n",
      "PRED   ..\n",
      "------------------------------------------------\n",
      "QUERY  sza-ni-im\n",
      "TARGET šanîm\n",
      "PRED   šanîm\n",
      "------------------------------------------------\n",
      "QUERY  sza pa-t,a-ri-szu\n",
      "TARGET ša paṭārišu\n",
      "PRED   ša paṭārim\n",
      "------------------------------------------------\n",
      "QUERY  a-na _sa10 kasz_\n",
      "TARGET ana šīm šikarim\n",
      "PRED   ana šakin ’ im\n",
      "------------------------------------------------\n",
      "QUERY  a-di bal#-t,u2 a-na is-qi2-szu li-kin-nu\n",
      "TARGET adi balțu ana isqišu likinnū\n",
      "PRED   ana atti šū\n",
      "------------------------------------------------\n",
      "QUERY  ik-szi-szu-ma _dumu-munus_-su a-na sza-ni-im it-ta-di-in\n",
      "TARGET ikšišuma mārassu ana šanîm ittadin\n",
      "PRED   mārassu ana tadāne\n",
      "------------------------------------------------\n",
      "QUERY  wa-ar-ka-sa3\n",
      "TARGET warkassa\n",
      "PRED   warkassa\n",
      "------------------------------------------------\n",
      "QUERY  _sze_ il-qe2\n",
      "TARGET še’am ilqe\n",
      "PRED   še ’ am ilteqe\n",
      "------------------------------------------------\n",
      "QUERY  a-ra-an di-nim szu-a-ti\n",
      "TARGET aran dīnim šuāti\n",
      "PRED   dīnim šuāti\n",
      "------------------------------------------------\n",
      "QUERY  um-ma at-ti-ma\n",
      "TARGET umma attima\n",
      "PRED   umma attima\n",
      "------------------------------------------------\n",
      "QUERY  szum-ma _lu2 sag-ir3 geme2 gu4_ u3 szi-ma-am ma-la i-ba-szu-u2\n",
      "TARGET šumma awīlum wardam amtam alpam u šīmam mala ibaššû\n",
      "PRED   šumma a ’ īlu ḫarīmta u sinnilte aššassu ezzib\n",
      "------------------------------------------------\n",
      "QUERY  sza _lugal_\n",
      "TARGET ša šarru\n",
      "PRED   ša šarrum\n",
      "------------------------------------------------\n",
      "QUERY  ra-mi-ni-sza a-szar pa-nu-sza-a-ni\n",
      "TARGET raminišama ašar pānūšani\n",
      "PRED   ašar tāḫazim ša akāle\n",
      "------------------------------------------------\n",
      "QUERY  {i7}buranun-na\n",
      "TARGET purattim\n",
      "PRED   kišād nār - šarri\n",
      "------------------------------------------------\n",
      "QUERY  um#-ma {munus}szi-ib-tu _geme2_-ka-a-[ma]\n",
      "TARGET umma šibtu amatkama\n",
      "PRED   umma šibtu amatkama\n",
      "------------------------------------------------\n",
      "QUERY  ik-ki-ru\n",
      "TARGET ikkiru\n",
      "PRED   šerikti\n",
      "------------------------------------------------\n",
      "QUERY  uz4-na-am sza-ak-na-ku-ma\n",
      "TARGET uznam šaknākuma\n",
      "PRED   uznam ša ina mūšim\n",
      "------------------------------------------------\n",
      "QUERY  la is,-s,a-bi-it\n",
      "TARGET lā iṣṣabit\n",
      "PRED   lā bīt\n",
      "------------------------------------------------\n",
      "QUERY  szum-ma _a-zu_\n",
      "TARGET šumma asûm\n",
      "PRED   šumma asûm\n",
      "------------------------------------------------\n",
      "QUERY  szum-ma a-wi-lum\n",
      "TARGET šumma awīlum\n",
      "PRED   šumma awīlum\n",
      "------------------------------------------------\n",
      "QUERY  mi-im-ma li-ib-bi\n",
      "TARGET mimma libbī\n",
      "PRED   mimma libbi\n",
      "------------------------------------------------\n",
      "QUERY  u3 _lu2-tur_-ka la-mi-ka li-bu-am\n",
      "TARGET u ṣuḫārka lamika libû’am\n",
      "PRED   u awīlī šarrāqīya\n",
      "------------------------------------------------\n",
      "QUERY  _szaman2-la2_ szu-u2\n",
      "TARGET šamallûm šū\n",
      "PRED   šamallûm šū\n",
      "------------------------------------------------\n",
      "QUERY  _dam_-su ni-ik-ta ep-pu-szu-u2-ni\n",
      "TARGET aššassu nīkta eppušuni\n",
      "PRED   aššassu mārašu itarru\n",
      "------------------------------------------------\n",
      "QUERY  _1(disz) iti u4-mesz_ szi-par2 _lugal_ e-pa-asz\n",
      "TARGET iltēn uraḫ umāte šipar šarre eppaš\n",
      "PRED   ilten uraḫ ūmāte šipar šarre eppaš\n",
      "------------------------------------------------\n",
      "QUERY  u3 i-na-an-na lu-uq-bi-ma\n",
      "TARGET u inanna luqbima\n",
      "PRED   u lū ina nabalkattim\n",
      "------------------------------------------------\n",
      "QUERY  u2-ul ib-ba-qar\n",
      "TARGET ul ibbaqqar\n",
      "PRED   ul ibbaqqar\n",
      "------------------------------------------------\n",
      "QUERY  u3 lu-u2 du-ul-la\n",
      "TARGET ū lū dulla\n",
      "PRED   u lū ul\n",
      "------------------------------------------------\n",
      "QUERY  _2(asz) gur mun_ a-na _1(disz) gin2 ku3-babbar_\n",
      "TARGET 2 kurrum ṭabtum ana 1 šiqil kaspim\n",
      "PRED   šina šiqil kaspam ana 1 šiqil kaspim\n",
      "------------------------------------------------\n",
      "QUERY  i-na u2-tu-lim\n",
      "TARGET ina utūlim\n",
      "PRED   ina utūlim\n",
      "------------------------------------------------\n",
      "QUERY  ep-sze-tim sza _a-sza3_ i-te-szu\n",
      "TARGET epšētim ša eqel itēšu\n",
      "PRED   ša eqel itēšu\n",
      "------------------------------------------------\n",
      "QUERY  na-ra-am {d}tu-tu\n",
      "TARGET narām tutu\n",
      "PRED   ištu innemdū\n",
      "------------------------------------------------\n",
      "QUERY  ma-li-ki-ia\n",
      "TARGET mālikīya\n",
      "PRED   kīma yâti\n",
      "------------------------------------------------\n",
      "QUERY  i-na qa2-ti-szu\n",
      "TARGET ina qātišu\n",
      "PRED   ina qātišu\n",
      "------------------------------------------------\n",
      "QUERY  is#?-ni-iq-szum\n",
      "TARGET isniqšum\n",
      "PRED   aššum riksa\n",
      "------------------------------------------------\n",
      "QUERY  t,up-pa-sza ki-i al-ma-te-ma i-szat,-t,u-ru\n",
      "TARGET ṭuppaša kî almattemma išaṭṭurū\n",
      "PRED   kīma sippim\n",
      "------------------------------------------------\n",
      "QUERY  szur-qa i-id-da-an\n",
      "TARGET šurqa iddan\n",
      "PRED   šurqa iddan\n",
      "------------------------------------------------\n",
      "QUERY  _nu-{gesz}kiri6_\n",
      "TARGET nukaribbum\n",
      "PRED   nukaribbum\n",
      "------------------------------------------------\n",
      "QUERY  1/3(disz) _ma-na ku3-babbar_\n",
      "TARGET šaluš mina kaspam\n",
      "PRED   1 / 3 mana kaspam\n",
      "------------------------------------------------\n",
      "QUERY  qi2-bi2-ma\n",
      "TARGET qibima\n",
      "PRED   qibima\n",
      "------------------------------------------------\n",
      "QUERY  a-ap-tum la na-as3-ha-at bu-sze-e ma-s,a-ar-tim\n",
      "TARGET aptum la nasḫat bušē maṣṣartim\n",
      "PRED   ana māt la tenê\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "QUERY  szum-ma ki-i _dam_-at _lu2_-ni la-a i-di\n",
      "TARGET šumma kî aššat a’īlenni la īde\n",
      "PRED   šumma kî aššat a ’ īlenni lā īde\n",
      "------------------------------------------------\n",
      "QUERY  wa-ar-ki-sza\n",
      "TARGET warkiša\n",
      "PRED   warki\n",
      "------------------------------------------------\n",
      "QUERY  i-na _iri_ za-rat-szim-ka2-me\n",
      "TARGET ina zārat-šim-ka-me\n",
      "PRED   ina bāb - râši\n",
      "------------------------------------------------\n",
      "QUERY  zi2-it-tam ki-ma\n",
      "TARGET zittam kīma\n",
      "PRED   kīma zittam\n",
      "------------------------------------------------\n",
      "QUERY  [isz]-tu qa-at,-t,u2-na-an{ki}\n",
      "TARGET ištu qaṭṭunan\n",
      "PRED   ištu māt qabrâ\n",
      "------------------------------------------------\n",
      "QUERY  im-ta-ha-ar\n",
      "TARGET imtaḫar\n",
      "PRED   itâr\n",
      "------------------------------------------------\n",
      "QUERY  a-na _e2_ e-mi-im\n",
      "TARGET ana bīt emim\n",
      "PRED   ana bīt emišu\n",
      "------------------------------------------------\n",
      "QUERY  sza a-na har-ra-an szar-ri-im\n",
      "TARGET ša ana ḫarrān šarrim\n",
      "PRED   ša ana ašar illiku\n",
      "------------------------------------------------\n",
      "QUERY  qi2-bi2-ma\n",
      "TARGET qibima\n",
      "PRED   qibima\n",
      "------------------------------------------------\n",
      "QUERY  _gissu bad3_ lu-u2 man-za-zu-ka\n",
      "TARGET ṣillī dūri lū manzazūka\n",
      "PRED   lū šareštu\n",
      "------------------------------------------------\n",
      "QUERY  szum-ma a-wi-lum\n",
      "TARGET šumma awīlum\n",
      "PRED   šumma awīlum\n",
      "------------------------------------------------\n",
      "QUERY  ma-ha-ar be-li2-ia a-sza-ka-an\n",
      "TARGET maḫar bēliya ašakkan\n",
      "PRED   bēlī ša maḫar ṣalmiya\n",
      "------------------------------------------------\n",
      "QUERY  a-na mu-ti-sza\n",
      "TARGET ana mutiša\n",
      "PRED   ana mutiša\n",
      "------------------------------------------------\n",
      "QUERY  it-ta-ki-ir\n",
      "TARGET ittakir\n",
      "PRED   ittakir\n",
      "------------------------------------------------\n",
      "QUERY  la i-le-i\n",
      "TARGET lā ile’i\n",
      "PRED   lā ile ’’ i\n",
      "------------------------------------------------\n",
      "QUERY  a-na sze20-bu-ul-tim\n",
      "TARGET ana šēbultim\n",
      "PRED   ana šēbultim\n",
      "------------------------------------------------\n",
      "QUERY  t,u-ur-rid-su-ma szu-s,i-szu2 ana lib3-bi _ka2_\n",
      "TARGET ṭurrissuma šūṣišu ana libbi bābi\n",
      "PRED   uṭarrissuma ulteṣišu ana bābi\n",
      "------------------------------------------------\n",
      "QUERY  ul-tu2 _sag-du_-szu2 a-di ki-bi-is _giri3_-szu2\n",
      "TARGET ultu qaqqadišu adi kibis šēpēšu\n",
      "PRED   adi ul ittē\n",
      "------------------------------------------------\n",
      "QUERY  sza2 {d}marduk rem-ni-i ka-bat-ta-szu2 ip-pa-asz2-ru\n",
      "TARGET ša marduk remni kabattašu ippašru\n",
      "PRED   marduk ša nagbe qāti ﻿ šu\n",
      "------------------------------------------------\n",
      "QUERY  _{gesz}tukul-mesz_-ia ip-par2-szi-du _giri3-mesz_-ia\n",
      "TARGET kakkīya ipparšidū šēpīya\n",
      "PRED   kakkī iššû\n",
      "------------------------------------------------\n",
      "QUERY  szi-ir ni-szi u2-t,i-ib\n",
      "TARGET šīr nišī uṭīb\n",
      "PRED   izzibši\n",
      "------------------------------------------------\n",
      "QUERY  _ARAD2_-ka-a-ma\n",
      "TARGET waradkama\n",
      "PRED   waradkama\n",
      "------------------------------------------------\n",
      "QUERY  li-ma-al-li-szu-ma\n",
      "TARGET limallīšuma\n",
      "PRED   šūma\n",
      "------------------------------------------------\n",
      "QUERY  szi-pi2-ir-szu\n",
      "TARGET šipiršu\n",
      "PRED   šipiršu\n",
      "------------------------------------------------\n",
      "QUERY  i-na pa-ni-szu ra-ma-an-sza\n",
      "TARGET ina pānišu ramānša\n",
      "PRED   ina pānišu ramānša\n",
      "------------------------------------------------\n",
      "QUERY  i-sza-ak-ka-an-ma\n",
      "TARGET išakkanma\n",
      "PRED   išakkakma\n",
      "------------------------------------------------\n",
      "QUERY  li-ma-al-li-szu-ma\n",
      "TARGET limallīšuma\n",
      "PRED   šūma\n",
      "------------------------------------------------\n",
      "QUERY  _dumu dumu_-e me-e-te sza 1(u) _mu-mesz_-szu-ni\n",
      "TARGET mār māre mēte ša ešer šanātišuni\n",
      "PRED   mārū māre ša ešer šanātišuni\n",
      "------------------------------------------------\n",
      "QUERY  ba-ab-tum a-na be-li2-szu u2-sze-di-ma _gu4_-szu la u2-sze-szi-ir-ma\n",
      "TARGET bābtum ana bēlišu ušēdima alapšu la ušēširma\n",
      "PRED   la ana bēlišu sullim\n",
      "------------------------------------------------\n",
      "QUERY  ma-ka-li el-lu-tim\n",
      "TARGET mākalī ellūtim\n",
      "PRED   ellūtim\n"
     ]
    }
   ],
   "source": [
    "def sample(num_samples=100):\n",
    "    for i in range(num_samples):\n",
    "        t = tests[random.randint(0, tests.num_rows)]\n",
    "    #     print(t)\n",
    "        src = t[\"source\"]\n",
    "        tgt = t[\"target\"]\n",
    "        query = src\n",
    "        pred = pipeline(query)[0][\"translation_text\"]\n",
    "        print(\"-\"*48)\n",
    "        print(\"QUERY \", query)\n",
    "        print(\"TARGET\", tgt)\n",
    "        print(\"PRED  \", pred)\n",
    "    #     break\n",
    "    \n",
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliterate All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8525d16a4c14d8f817f56f092a18280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t in tqdm(all_translations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
